{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = pd.read_csv(\"~/mt-dnn/toxic_data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(src,test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_tokenizer(raw_string):\n",
    "    raw_string=raw_string[0]\n",
    "    tokens = tokenizer.tokenize(raw_string)\n",
    "    if len(tokens)>510:\n",
    "        tokens = tokens[0:510]\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxic_prep(sample,path_to_prepared=''):\n",
    "    how_many_datapoints = sample.shape[0]\n",
    "    sample = sample[['id',\"target\",'comment_text']]        \n",
    "    sample['token_id'] = sample[['comment_text']].apply(full_tokenizer,axis = 1)\n",
    "    sample['type_id'] = sample[['token_id']].apply(lambda x:[0]*len(x[0]),axis = 1)\n",
    "    sample.drop(['comment_text'], axis=1, inplace=True)\n",
    "    sample = sample.rename(columns={'id': 'uid', 'target': 'label'})\n",
    "    sample.to_json(path_to_prepared, orient='records', lines=True)\n",
    "toxic_prep(test,\"/home/jupyter/mt-dnn/data/mt_dnn/toxic_test.json\")\n",
    "toxic_prep(train,\"/home/jupyter/mt-dnn/data/mt_dnn/toxic_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90244"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_len(\"/home/jupyter/mt-dnn/data/mt_dnn/toxic_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1714630"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_len(\"/home/jupyter/mt-dnn/data/mt_dnn/toxic_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_file(file,max_length):\n",
    "    with open(file) as f, open(\"/home/jupyter/mt-dnn/tmp.txt\", \"w\") as out:\n",
    "        out.writelines(f.readlines()[:max_length])\n",
    "    os.remove(file)\n",
    "    shutil.move(\"/home/jupyter/mt-dnn/tmp.txt\",file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying a shorter train set, for more reasonable training time\n",
    "cut_file('/home/jupyter/mt-dnn/data/mt_dnn/toxic_train.json',214328)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not working... Cuda ran out of memory after 3 hours...\n",
    "#I'll try instead to divide the train set into much smaller batches, to have shorter epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smaller_train_sets(file,file_size=1714630,factor=30):\n",
    "    with open(file) as f:\n",
    "        lines=f.readlines()\n",
    "        mini_train_size = int(file_size/factor)\n",
    "        for i in range(factor):\n",
    "            with open(\"/home/jupyter/mt-dnn/data/mt_dnn/toxic_train\" + str(i) + \".json\", \"w\") as out:\n",
    "                out.writelines(lines[i*mini_train_size:(i+1)*mini_train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_train_sets(\"/home/jupyter/mt-dnn/data/mt_dnn/toxic_train_full.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0]*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
