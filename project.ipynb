{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test GPU:\n",
    "# import torch\n",
    "# torch.cuda.current_device()\n",
    "# torch.cuda.device(0)\n",
    "# torch.cuda.device_count()\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mt-dnn\n"
     ]
    }
   ],
   "source": [
    "# should be in /home/jupyter/mt-dnn\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_data_proc_512.log\tdata_utils   module\t    project.ipynb     scripts\n",
      "checkpoints\t\tdocker\t     mt_dnn\t    README.md\t      TEST.txt\n",
      "config\t\t\tdownload.sh  mt_dnn_models  requirements.txt  train.py\n",
      "data\t\t\tLICENSE      prepro.py\t    run_toy.sh\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.3\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.16.2)\n",
      "Collecting torch==0.4.1.post2 (from -r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/91/1b2871d6c8ca079254deae5872af32e02e9a85f07dd0834e8b3489ce138f/torch-0.4.1.post2-cp37-cp37m-manylinux1_x86_64.whl (519.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 519.5MB 90kB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (4.31.1)\n",
      "Requirement already satisfied: colorlog in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (4.0.2)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.9.151)\n",
      "Requirement already satisfied: pytorch-pretrained-bert==v0.6.0 in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.151 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 5)) (1.12.151)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 5)) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (2.21.0)\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.151->boto3->-r requirements.txt (line 5)) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.151->boto3->-r requirements.txt (line 5)) (2.8.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20; python_version >= \"3.4\" in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.151->boto3->-r requirements.txt (line 5)) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.151->boto3->-r requirements.txt (line 5)) (1.10.0)\n",
      "\u001b[31mfastai 1.0.52 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[31mfastai 1.0.52 has requirement torch>=1.0.0, but you'll have torch 0.4.1.post2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch\n",
      "  Found existing installation: torch 0.4.1\n",
      "    Uninstalling torch-0.4.1:\n",
      "      Successfully uninstalled torch-0.4.1\n",
      "Successfully installed torch-0.4.1.post2\n"
     ]
    }
   ],
   "source": [
    "#I've changed requirements.txt to use pytorch 0.4.1.post2 instead of 0.4.1 \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a folder /home/jupyter/mt-dnn/data\n",
      "mkdir: cannot create directory ‘/home/jupyter/mt-dnn/data’: File exists\n",
      "Create a folder BERT_DIR\n",
      "mkdir: cannot create directory ‘/home/jupyter/mt-dnn/mt_dnn_models’: File exists\n",
      "Cloning into 'jiant'...\n",
      "remote: Enumerating objects: 214, done.\u001b[K\n",
      "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
      "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
      "remote: Total 13408 (delta 142), reused 121 (delta 64), pack-reused 13194\u001b[K\n",
      "Receiving objects: 100% (13408/13408), 3.75 MiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (9899/9899), done.\n",
      "Downloading and extracting CoLA...\n",
      "\tCompleted!\n",
      "Downloading and extracting SST...\n",
      "\tCompleted!\n",
      "Processing MRPC...\n",
      "Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
      "\tCompleted!\n",
      "Downloading and extracting QQP...\n",
      "\tCompleted!\n",
      "Downloading and extracting STS...\n",
      "\tCompleted!\n",
      "Downloading and extracting MNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting SNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting QNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting RTE...\n",
      "\tCompleted!\n",
      "Downloading and extracting WNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting diagnostic data...\n",
      "\tCompleted!\n",
      "--2019-05-27 01:35:45--  http://data.allenai.org.s3.amazonaws.com/downloads/SciTailV1.1.zip\n",
      "Resolving data.allenai.org.s3.amazonaws.com (data.allenai.org.s3.amazonaws.com)... 52.218.245.146\n",
      "Connecting to data.allenai.org.s3.amazonaws.com (data.allenai.org.s3.amazonaws.com)|52.218.245.146|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14174621 (14M) [application/zip]\n",
      "Saving to: ‘SciTailV1.1.zip’\n",
      "\n",
      "SciTailV1.1.zip     100%[===================>]  13.52M  11.1MB/s    in 1.2s    \n",
      "\n",
      "2019-05-27 01:35:47 (11.1 MB/s) - ‘SciTailV1.1.zip’ saved [14174621/14174621]\n",
      "\n",
      "Archive:  SciTailV1.1.zip\n",
      "   creating: SciTailV1.1/\n",
      "   creating: SciTailV1.1/snli_format/\n",
      "  inflating: SciTailV1.1/snli_format/scitail_1.0_test.txt  \n",
      "  inflating: SciTailV1.1/snli_format/scitail_1.0_train.txt  \n",
      "  inflating: SciTailV1.1/snli_format/scitail_1.0_dev.txt  \n",
      "  inflating: SciTailV1.1/snli_format/README.txt  \n",
      "   creating: SciTailV1.1/dgem_format/\n",
      "  inflating: SciTailV1.1/dgem_format/scitail_1.0_structure_dev.tsv  \n",
      "  inflating: SciTailV1.1/dgem_format/scitail_1.0_structure_train.tsv  \n",
      "  inflating: SciTailV1.1/dgem_format/README.txt  \n",
      "  inflating: SciTailV1.1/dgem_format/scitail_1.0_structure_test.tsv  \n",
      "   creating: SciTailV1.1/predictor_format/\n",
      "  inflating: SciTailV1.1/predictor_format/scitail_1.0_structure_dev.jsonl  \n",
      "  inflating: SciTailV1.1/predictor_format/scitail_1.0_structure_test.jsonl  \n",
      "  inflating: SciTailV1.1/predictor_format/README.txt  \n",
      "  inflating: SciTailV1.1/predictor_format/scitail_1.0_structure_train.jsonl  \n",
      "  inflating: SciTailV1.1/all_annotations.tsv  \n",
      "  inflating: SciTailV1.1/README.txt  \n",
      "   creating: SciTailV1.1/tsv_format/\n",
      "  inflating: SciTailV1.1/tsv_format/scitail_1.0_test.tsv  \n",
      "  inflating: SciTailV1.1/tsv_format/scitail_1.0_train.tsv  \n",
      "  inflating: SciTailV1.1/tsv_format/scitail_1.0_dev.tsv  \n",
      "--2019-05-27 01:35:47--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 2607:f8b0:400e:c09::80\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 407727028 (389M) [application/zip]\n",
      "Saving to: ‘uncased_bert_base.zip’\n",
      "\n",
      "uncased_bert_base.z 100%[===================>] 388.84M  83.1MB/s    in 4.8s    \n",
      "\n",
      "2019-05-27 01:35:52 (80.8 MB/s) - ‘uncased_bert_base.zip’ saved [407727028/407727028]\n",
      "\n",
      "Archive:  uncased_bert_base.zip\n",
      "   creating: uncased_L-12_H-768_A-12/\n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n",
      "--2019-05-27 01:35:57--  https://mrc.blob.core.windows.net/mt-dnn-model/bert_model_base_v2.pt\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 437961819 (418M) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/mt-dnn/mt_dnn_models/bert_model_base.pt’\n",
      "\n",
      "/home/jupyter/mt-dn 100%[===================>] 417.67M  9.18MB/s    in 47s     \n",
      "\n",
      "2019-05-27 01:36:44 (8.97 MB/s) - ‘/home/jupyter/mt-dnn/mt_dnn_models/bert_model_base.pt’ saved [437961819/437961819]\n",
      "\n",
      "--2019-05-27 01:36:44--  https://mrc.blob.core.windows.net/mt-dnn-model/bert_model_large_v2.pt\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1340632883 (1.2G) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/mt-dnn/mt_dnn_models/bert_model_large.pt’\n",
      "\n",
      "/home/jupyter/mt-dn 100%[===================>]   1.25G  4.76MB/s    in 2m 45s  \n",
      "\n",
      "2019-05-27 01:39:30 (7.74 MB/s) - ‘/home/jupyter/mt-dnn/mt_dnn_models/bert_model_large.pt’ saved [1340632883/1340632883]\n",
      "\n",
      "--2019-05-27 01:39:30--  https://mrc.blob.core.windows.net/mt-dnn-model/mt_dnn_base.pt\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 437962060 (418M) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/mt-dnn/mt_dnn_models/mt_dnn_base.pt’\n",
      "\n",
      "/home/jupyter/mt-dn 100%[===================>] 417.67M  2.14MB/s    in 86s     \n",
      "\n",
      "2019-05-27 01:40:56 (4.88 MB/s) - ‘/home/jupyter/mt-dnn/mt_dnn_models/mt_dnn_base.pt’ saved [437962060/437962060]\n",
      "\n",
      "--2019-05-27 01:40:56--  https://mrc.blob.core.windows.net/mt-dnn-model/mt_dnn_large.pt\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1340632883 (1.2G) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/mt-dnn/mt_dnn_models/mt_dnn_large.pt’\n",
      "\n",
      "/home/jupyter/mt-dn 100%[===================>]   1.25G  10.2MB/s    in 2m 33s  \n",
      "\n",
      "2019-05-27 01:43:30 (8.37 MB/s) - ‘/home/jupyter/mt-dnn/mt_dnn_models/mt_dnn_large.pt’ saved [1340632883/1340632883]\n",
      "\n",
      "Create a folder /home/jupyter/mt-dnn/data\n",
      "mkdir: cannot create directory ‘domain_adaptation’: File exists\n",
      "--2019-05-27 01:43:30--  https://mrc.blob.core.windows.net/mt-dnn-model/data.zip\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 57236306 (55M) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>]  54.58M  12.6MB/s    in 3.7s    \n",
      "\n",
      "2019-05-27 01:43:34 (14.6 MB/s) - ‘data.zip’ saved [57236306/57236306]\n",
      "\n",
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/scitail_001_train.json  \n",
      "  inflating: data/scitail_01_train.json  \n",
      "  inflating: data/scitail_1_train.json  \n",
      "  inflating: data/scitail_5_train.json  \n",
      "  inflating: data/scitail_dev.json   \n",
      "  inflating: data/scitail_test.json  \n",
      "  inflating: data/scitail_train.json  \n",
      "  inflating: data/scitail_train_shuff.json  \n",
      "  inflating: data/snli_001_train.json  \n",
      "  inflating: data/snli_01_train.json  \n",
      "  inflating: data/snli_1_train.json  \n",
      "  inflating: data/snli_5_train.json  \n",
      "  inflating: data/snli_dev.json      \n",
      "  inflating: data/snli_test.json     \n",
      "  inflating: data/snli_train.json    \n",
      "  inflating: data/snli_train_shuff.json  \n"
     ]
    }
   ],
   "source": [
    "# !sh download.sh\n",
    "# note: The wrong QNLI dataset is downloaded right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "05/27/2019 04:05:13 Loaded 23596 SciTail train samples\n",
      "05/27/2019 04:05:13 Loaded 1304 SciTail dev samples\n",
      "05/27/2019 04:05:13 Loaded 2126 SciTail test samples\n",
      "05/27/2019 04:05:15 Loaded 549367 SNLI train samples\n",
      "05/27/2019 04:05:15 Loaded 9842 SNLI dev samples\n",
      "05/27/2019 04:05:15 Loaded 9824 SNLI test samples\n",
      "05/27/2019 04:05:17 Loaded 392702 MNLI train samples\n",
      "05/27/2019 04:05:17 Loaded 9815 MNLI matched dev samples\n",
      "05/27/2019 04:05:17 Loaded 9832 MNLI mismatched dev samples\n",
      "05/27/2019 04:05:17 Loaded 9796 MNLI matched test samples\n",
      "05/27/2019 04:05:17 Loaded 9847 MNLI mismatched test samples\n",
      "05/27/2019 04:05:17 Loaded 3668 MRPC train samples\n",
      "05/27/2019 04:05:17 Loaded 408 MRPC dev samples\n",
      "05/27/2019 04:05:17 Loaded 1725 MRPC test samples\n",
      "05/27/2019 04:05:17 Loaded 104743 QNLI train samples\n",
      "05/27/2019 04:05:17 Loaded 5463 QNLI dev samples\n",
      "05/27/2019 04:05:17 Loaded 5463 QNLI test samples\n",
      "05/27/2019 04:05:17 Loaded 104743 QNLI train samples\n",
      "05/27/2019 04:05:17 Loaded 5463 QNLI dev samples\n",
      "05/27/2019 04:05:17 Loaded 5463 QNLI test samples\n",
      "05/27/2019 04:05:19 Loaded 363849 QQP train samples\n",
      "05/27/2019 04:05:19 Loaded 40430 QQP dev samples\n",
      "05/27/2019 04:05:19 Loaded 390965 QQP test samples\n",
      "05/27/2019 04:05:19 Loaded 2490 RTE train samples\n",
      "05/27/2019 04:05:19 Loaded 277 RTE dev samples\n",
      "05/27/2019 04:05:19 Loaded 3000 RTE test samples\n",
      "05/27/2019 04:05:19 Loaded 635 WNLI train samples\n",
      "05/27/2019 04:05:19 Loaded 71 WNLI dev samples\n",
      "05/27/2019 04:05:19 Loaded 146 WNLI test samples\n",
      "05/27/2019 04:05:19 Loaded 67349 SST train samples\n",
      "05/27/2019 04:05:19 Loaded 872 SST dev samples\n",
      "05/27/2019 04:05:19 Loaded 1821 SST test samples\n",
      "05/27/2019 04:05:19 Loaded 5749 STS-B train samples\n",
      "05/27/2019 04:05:19 Loaded 1500 STS-B dev samples\n",
      "05/27/2019 04:05:19 Loaded 1379 STS-B test samples\n",
      "05/27/2019 04:05:19 Loaded 8550 COLA train samples\n",
      "05/27/2019 04:05:19 Loaded 1042 COLA dev samples\n",
      "05/27/2019 04:05:19 Loaded 1063 COLA test samples\n",
      "05/27/2019 04:05:34 done with scitail\n",
      "05/27/2019 04:08:43 done with snli\n",
      "05/27/2019 04:12:32 done with mnli\n",
      "05/27/2019 04:12:37 done with mrpc\n",
      "05/27/2019 04:13:56 done with qnli\n",
      "05/27/2019 04:13:56 done with qnli\n",
      "05/27/2019 04:19:08 done with qqp\n",
      "05/27/2019 04:19:13 done with rte\n",
      "05/27/2019 04:19:13 done with wnli\n",
      "05/27/2019 04:19:17 done with stsb\n",
      "05/27/2019 04:19:29 done with sst\n",
      "05/27/2019 04:19:31 done with cola\n"
     ]
    }
   ],
   "source": [
    "!python prepro.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Namespace(answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='data/mt_dnn', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, epochs=1, freeze_layers=-1, global_grad_clipping=1.0, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='mt_dnn_models/mt_dnn_base.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoint', pw_tasks=['qnnli'], ratio=0, scheduler_type='ms', seed=2018, task_config_path='configs/tasks_config.json', test_datasets=['mnli_mismatched', 'mnli_matched'], train_datasets=['mnli'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)\n",
      "05/27/2019 07:43:23 0\n",
      "05/27/2019 07:43:23 Launching the MT-DNN training\n",
      "05/27/2019 07:43:23 Loading data/mt_dnn/mnli_train.json as task 0\n",
      "Loaded 392702 samples out of 392702\n",
      "05/27/2019 07:43:31 3\n",
      "Loaded 9832 samples out of 9832\n",
      "Loaded 9847 samples out of 9847\n",
      "Loaded 9815 samples out of 9815\n",
      "Loaded 9796 samples out of 9796\n",
      "05/27/2019 07:43:33 ####################\n",
      "05/27/2019 07:43:33 {'log_file': 'mt-dnn-train.log', 'init_checkpoint': 'mt_dnn_models/mt_dnn_base.pt', 'data_dir': 'data/mt_dnn', 'data_sort_on': False, 'name': 'farmer', 'train_datasets': ['mnli'], 'test_datasets': ['mnli_mismatched', 'mnli_matched'], 'pw_tasks': ['qnnli'], 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': [0], 'label_size': '3', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'cuda': True, 'log_per_updates': 500, 'epochs': 1, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'ema_opt': 0, 'ema_gamma': 0.995, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': 'checkpoint', 'seed': 2018, 'task_config_path': 'configs/tasks_config.json', 'tasks_dropout_p': [0.1]}\n",
      "05/27/2019 07:43:33 ####################\n",
      "05/27/2019 07:43:39 \n",
      "############# Model Arch of MT-DNN #############\n",
      "SANBertNetwork(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (scoring_list): ModuleList(\n",
      "    (0): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "05/27/2019 07:43:39 Total number of params: 109484547\n",
      "05/27/2019 07:43:42 At epoch 0\n",
      "05/27/2019 07:43:42 Task [ 0] updates[     1] train loss[0.90238] remaining[3:50:12]\n",
      "05/27/2019 07:44:58 Task [ 0] updates[   500] train loss[0.80636] remaining[2:03:26]\n",
      "05/27/2019 07:46:13 Task [ 0] updates[  1000] train loss[0.55786] remaining[2:01:27]\n",
      "05/27/2019 07:47:29 Task [ 0] updates[  1500] train loss[0.44972] remaining[2:00:21]\n",
      "05/27/2019 07:48:45 Task [ 0] updates[  2000] train loss[0.39096] remaining[1:58:48]\n",
      "05/27/2019 07:49:59 Task [ 0] updates[  2500] train loss[0.36466] remaining[1:57:16]\n",
      "05/27/2019 07:51:15 Task [ 0] updates[  3000] train loss[0.34674] remaining[1:55:56]\n",
      "05/27/2019 07:52:30 Task [ 0] updates[  3500] train loss[0.33618] remaining[1:54:36]\n",
      "05/27/2019 07:53:45 Task [ 0] updates[  4000] train loss[0.32629] remaining[1:53:19]\n",
      "05/27/2019 07:55:01 Task [ 0] updates[  4500] train loss[0.32255] remaining[1:52:06]\n",
      "05/27/2019 07:56:16 Task [ 0] updates[  5000] train loss[0.32050] remaining[1:50:49]\n",
      "05/27/2019 07:57:31 Task [ 0] updates[  5500] train loss[0.31832] remaining[1:49:34]\n",
      "05/27/2019 07:58:47 Task [ 0] updates[  6000] train loss[0.31900] remaining[1:48:20]\n",
      "05/27/2019 08:00:03 Task [ 0] updates[  6500] train loss[0.31736] remaining[1:47:09]\n",
      "05/27/2019 08:01:19 Task [ 0] updates[  7000] train loss[0.31543] remaining[1:45:56]\n",
      "05/27/2019 08:02:34 Task [ 0] updates[  7500] train loss[0.31597] remaining[1:44:39]\n",
      "05/27/2019 08:03:49 Task [ 0] updates[  8000] train loss[0.31507] remaining[1:43:21]\n",
      "05/27/2019 08:05:04 Task [ 0] updates[  8500] train loss[0.31381] remaining[1:42:01]\n",
      "05/27/2019 08:06:18 Task [ 0] updates[  9000] train loss[0.31364] remaining[1:40:40]\n",
      "05/27/2019 08:07:33 Task [ 0] updates[  9500] train loss[0.31489] remaining[1:39:25]\n",
      "05/27/2019 08:08:48 Task [ 0] updates[ 10000] train loss[0.31472] remaining[1:38:09]\n",
      "05/27/2019 08:10:03 Task [ 0] updates[ 10500] train loss[0.31525] remaining[1:36:51]\n",
      "05/27/2019 08:11:18 Task [ 0] updates[ 11000] train loss[0.31620] remaining[1:35:34]\n",
      "05/27/2019 08:12:33 Task [ 0] updates[ 11500] train loss[0.31627] remaining[1:34:20]\n",
      "05/27/2019 08:13:49 Task [ 0] updates[ 12000] train loss[0.31693] remaining[1:33:06]\n",
      "05/27/2019 08:15:04 Task [ 0] updates[ 12500] train loss[0.31662] remaining[1:31:48]\n",
      "05/27/2019 08:16:17 Task [ 0] updates[ 13000] train loss[0.31613] remaining[1:30:27]\n",
      "05/27/2019 08:17:32 Task [ 0] updates[ 13500] train loss[0.31684] remaining[1:29:11]\n",
      "05/27/2019 08:18:46 Task [ 0] updates[ 14000] train loss[0.31703] remaining[1:27:54]\n",
      "05/27/2019 08:20:02 Task [ 0] updates[ 14500] train loss[0.31781] remaining[1:26:39]\n",
      "05/27/2019 08:21:16 Task [ 0] updates[ 15000] train loss[0.31817] remaining[1:25:22]\n",
      "05/27/2019 08:22:30 Task [ 0] updates[ 15500] train loss[0.31873] remaining[1:24:05]\n",
      "05/27/2019 08:23:45 Task [ 0] updates[ 16000] train loss[0.31857] remaining[1:22:50]\n",
      "05/27/2019 08:25:00 Task [ 0] updates[ 16500] train loss[0.31904] remaining[1:21:33]\n",
      "05/27/2019 08:26:14 Task [ 0] updates[ 17000] train loss[0.31871] remaining[1:20:17]\n",
      "05/27/2019 08:27:30 Task [ 0] updates[ 17500] train loss[0.31829] remaining[1:19:03]\n",
      "05/27/2019 08:28:45 Task [ 0] updates[ 18000] train loss[0.31794] remaining[1:17:49]\n",
      "05/27/2019 08:30:00 Task [ 0] updates[ 18500] train loss[0.31859] remaining[1:16:33]\n",
      "05/27/2019 08:31:15 Task [ 0] updates[ 19000] train loss[0.31816] remaining[1:15:18]\n",
      "05/27/2019 08:32:31 Task [ 0] updates[ 19500] train loss[0.31833] remaining[1:14:03]\n",
      "05/27/2019 08:33:45 Task [ 0] updates[ 20000] train loss[0.31851] remaining[1:12:48]\n",
      "05/27/2019 08:35:00 Task [ 0] updates[ 20500] train loss[0.31890] remaining[1:11:32]\n",
      "05/27/2019 08:36:16 Task [ 0] updates[ 21000] train loss[0.31921] remaining[1:10:18]\n",
      "05/27/2019 08:37:32 Task [ 0] updates[ 21500] train loss[0.31896] remaining[1:09:04]\n",
      "05/27/2019 08:38:48 Task [ 0] updates[ 22000] train loss[0.31937] remaining[1:07:50]\n",
      "05/27/2019 08:40:03 Task [ 0] updates[ 22500] train loss[0.31888] remaining[1:06:35]\n",
      "05/27/2019 08:41:18 Task [ 0] updates[ 23000] train loss[0.31859] remaining[1:05:19]\n",
      "05/27/2019 08:42:34 Task [ 0] updates[ 23500] train loss[0.31877] remaining[1:04:05]\n",
      "05/27/2019 08:43:49 Task [ 0] updates[ 24000] train loss[0.31850] remaining[1:02:51]\n",
      "05/27/2019 08:45:04 Task [ 0] updates[ 24500] train loss[0.31821] remaining[1:01:35]\n",
      "05/27/2019 08:46:19 Task [ 0] updates[ 25000] train loss[0.31785] remaining[1:00:19]\n",
      "05/27/2019 08:47:34 Task [ 0] updates[ 25500] train loss[0.31747] remaining[0:59:04]\n",
      "05/27/2019 08:48:48 Task [ 0] updates[ 26000] train loss[0.31729] remaining[0:57:49]\n",
      "05/27/2019 08:50:03 Task [ 0] updates[ 26500] train loss[0.31765] remaining[0:56:33]\n",
      "05/27/2019 08:51:18 Task [ 0] updates[ 27000] train loss[0.31745] remaining[0:55:18]\n",
      "05/27/2019 08:52:32 Task [ 0] updates[ 27500] train loss[0.31755] remaining[0:54:02]\n",
      "05/27/2019 08:53:46 Task [ 0] updates[ 28000] train loss[0.31726] remaining[0:52:46]\n",
      "05/27/2019 08:55:01 Task [ 0] updates[ 28500] train loss[0.31669] remaining[0:51:31]\n",
      "05/27/2019 08:56:17 Task [ 0] updates[ 29000] train loss[0.31653] remaining[0:50:16]\n",
      "05/27/2019 08:57:30 Task [ 0] updates[ 29500] train loss[0.31668] remaining[0:49:00]\n",
      "05/27/2019 08:58:45 Task [ 0] updates[ 30000] train loss[0.31642] remaining[0:47:44]\n",
      "05/27/2019 08:59:59 Task [ 0] updates[ 30500] train loss[0.31623] remaining[0:46:29]\n",
      "05/27/2019 09:01:14 Task [ 0] updates[ 31000] train loss[0.31557] remaining[0:45:14]\n",
      "05/27/2019 09:02:29 Task [ 0] updates[ 31500] train loss[0.31542] remaining[0:43:59]\n",
      "05/27/2019 09:03:45 Task [ 0] updates[ 32000] train loss[0.31536] remaining[0:42:44]\n",
      "05/27/2019 09:05:00 Task [ 0] updates[ 32500] train loss[0.31512] remaining[0:41:29]\n",
      "05/27/2019 09:06:14 Task [ 0] updates[ 33000] train loss[0.31465] remaining[0:40:14]\n",
      "05/27/2019 09:07:29 Task [ 0] updates[ 33500] train loss[0.31429] remaining[0:38:59]\n",
      "05/27/2019 09:08:44 Task [ 0] updates[ 34000] train loss[0.31397] remaining[0:37:43]\n",
      "05/27/2019 09:09:59 Task [ 0] updates[ 34500] train loss[0.31384] remaining[0:36:29]\n",
      "05/27/2019 09:11:15 Task [ 0] updates[ 35000] train loss[0.31385] remaining[0:35:14]\n",
      "05/27/2019 09:12:30 Task [ 0] updates[ 35500] train loss[0.31347] remaining[0:33:59]\n",
      "05/27/2019 09:13:46 Task [ 0] updates[ 36000] train loss[0.31306] remaining[0:32:44]\n",
      "05/27/2019 09:15:01 Task [ 0] updates[ 36500] train loss[0.31324] remaining[0:31:29]\n",
      "05/27/2019 09:16:16 Task [ 0] updates[ 37000] train loss[0.31318] remaining[0:30:14]\n",
      "05/27/2019 09:17:31 Task [ 0] updates[ 37500] train loss[0.31259] remaining[0:28:59]\n",
      "05/27/2019 09:18:46 Task [ 0] updates[ 38000] train loss[0.31257] remaining[0:27:44]\n",
      "05/27/2019 09:20:02 Task [ 0] updates[ 38500] train loss[0.31216] remaining[0:26:29]\n",
      "05/27/2019 09:21:17 Task [ 0] updates[ 39000] train loss[0.31203] remaining[0:25:14]\n",
      "05/27/2019 09:22:33 Task [ 0] updates[ 39500] train loss[0.31182] remaining[0:23:59]\n",
      "05/27/2019 09:23:48 Task [ 0] updates[ 40000] train loss[0.31181] remaining[0:22:44]\n",
      "05/27/2019 09:25:03 Task [ 0] updates[ 40500] train loss[0.31148] remaining[0:21:29]\n",
      "05/27/2019 09:26:18 Task [ 0] updates[ 41000] train loss[0.31074] remaining[0:20:14]\n",
      "05/27/2019 09:27:34 Task [ 0] updates[ 41500] train loss[0.31062] remaining[0:18:59]\n",
      "05/27/2019 09:28:48 Task [ 0] updates[ 42000] train loss[0.31016] remaining[0:17:44]\n",
      "05/27/2019 09:30:05 Task [ 0] updates[ 42500] train loss[0.30973] remaining[0:16:29]\n",
      "05/27/2019 09:31:20 Task [ 0] updates[ 43000] train loss[0.30962] remaining[0:15:14]\n",
      "05/27/2019 09:32:37 Task [ 0] updates[ 43500] train loss[0.30954] remaining[0:13:59]\n",
      "05/27/2019 09:33:52 Task [ 0] updates[ 44000] train loss[0.30942] remaining[0:12:44]\n",
      "05/27/2019 09:35:07 Task [ 0] updates[ 44500] train loss[0.30920] remaining[0:11:29]\n",
      "05/27/2019 09:36:22 Task [ 0] updates[ 45000] train loss[0.30882] remaining[0:10:14]\n",
      "05/27/2019 09:37:37 Task [ 0] updates[ 45500] train loss[0.30826] remaining[0:08:58]\n",
      "05/27/2019 09:38:51 Task [ 0] updates[ 46000] train loss[0.30814] remaining[0:07:43]\n",
      "05/27/2019 09:40:04 Task [ 0] updates[ 46500] train loss[0.30812] remaining[0:06:28]\n",
      "05/27/2019 09:41:20 Task [ 0] updates[ 47000] train loss[0.30765] remaining[0:05:13]\n",
      "05/27/2019 09:42:35 Task [ 0] updates[ 47500] train loss[0.30739] remaining[0:03:58]\n",
      "05/27/2019 09:43:51 Task [ 0] updates[ 48000] train loss[0.30699] remaining[0:02:43]\n",
      "05/27/2019 09:45:07 Task [ 0] updates[ 48500] train loss[0.30686] remaining[0:01:28]\n",
      "05/27/2019 09:46:23 Task [ 0] updates[ 49000] train loss[0.30684] remaining[0:00:13]\n",
      "05/27/2019 09:47:12 Task mnli_mismatched -- epoch 0 -- Dev ACC: 84.378\n",
      "05/27/2019 09:47:47 [new test scores saved.]\n",
      "05/27/2019 09:48:22 Task mnli_matched -- epoch 0 -- Dev ACC: 84.249\n",
      "05/27/2019 09:48:57 [new test scores saved.]\n"
     ]
    }
   ],
   "source": [
    "!python train.py --epochs  --init_checkpoint mt_dnn_models/mt_dnn_base.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripts/run_mt_dnn.sh: 2: scripts/run_mt_dnn.sh: [[: not found\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Namespace(answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=1, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=32, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='../data/mt_dnn', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, epochs=5, freeze_layers=-1, global_grad_clipping=1.0, grad_clipping=0.0, have_lr_scheduler=True, init_checkpoint='../mt_dnn_models/bert_model_large.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='checkpoints/mt-dnn-rte_adamax_answer_opt1_gc0_ggc1_2019-05-18T0717/log.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, momentum=0, mtl_opt=0, multi_gpu_on=True, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoints/mt-dnn-rte_adamax_answer_opt1_gc0_ggc1_2019-05-18T0717', pw_tasks=['qnnli'], ratio=0, scheduler_type='ms', seed=2018, task_config_path='configs/tasks_config.json', test_datasets=['mnli_matched', 'mnli_mismatched', 'rte'], train_datasets=['mnli', 'rte', 'qqp', 'qnli', 'mrpc', 'sst', 'cola', 'stsb'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)\n",
      "05/18/2019 07:17:19 1\n",
      "05/18/2019 07:17:19 Launching the MT-DNN training\n",
      "05/18/2019 07:17:19 Loading ../data/mt_dnn/mnli_train.json as task 0\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 353, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 181, in main\n",
      "    train_data = BatchGen(BatchGen.load(train_path, True, pairwise=pw_task, maxlen=args.max_seq_len),\n",
      "  File \"/home/jupyter/mt-dnn/mt_dnn/batcher.py\", line 52, in load\n",
      "    with open(path, 'r', encoding='utf-8') as reader:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/mt_dnn/mnli_train.json'\n"
     ]
    }
   ],
   "source": [
    "!sh scripts/run_mt_dnn.sh 32 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripts/run_stsb.sh: 2: scripts/run_stsb.sh: [[: not found\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Namespace(answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=32, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='../data/mt_dnn', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, epochs=5, freeze_layers=-1, global_grad_clipping=1.0, grad_clipping=0.0, have_lr_scheduler=True, init_checkpoint='../mt_dnn_models/mt_dnn_large.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='checkpoints/mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-27T0125/log.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoints/mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-27T0125', pw_tasks=['qnnli'], ratio=0, scheduler_type='ms', seed=2018, task_config_path='configs/tasks_config.json', test_datasets=['stsb'], train_datasets=['stsb'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)\n",
      "05/27/2019 01:25:58 0\n",
      "05/27/2019 01:25:58 Launching the MT-DNN training\n",
      "05/27/2019 01:25:58 Loading ../data/mt_dnn/stsb_train.json as task 0\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 353, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 181, in main\n",
      "    train_data = BatchGen(BatchGen.load(train_path, True, pairwise=pw_task, maxlen=args.max_seq_len),\n",
      "  File \"/home/jupyter/mt-dnn/mt_dnn/batcher.py\", line 52, in load\n",
      "    with open(path, 'r', encoding='utf-8') as reader:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/mt_dnn/stsb_train.json'\n"
     ]
    }
   ],
   "source": [
    "!sh scripts/run_stsb.sh 32 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
