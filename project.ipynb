{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test GPU:\n",
    "# import torch\n",
    "# torch.cuda.current_device()\n",
    "# torch.cuda.device(0)\n",
    "# torch.cuda.device_count()\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mt-dnn\n"
     ]
    }
   ],
   "source": [
    "# should be in /home/jupyter/mt-dnn\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_data_proc_512.log\tdata_utils   module\t    project.ipynb     scripts\n",
      "checkpoints\t\tdocker\t     mt_dnn\t    README.md\t      TEST.txt\n",
      "config\t\t\tdownload.sh  mt_dnn_models  requirements.txt  train.py\n",
      "data\t\t\tLICENSE      prepro.py\t    run_toy.sh\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.3\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.16.2)\n",
      "Requirement already satisfied: torch==0.4.1 in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (4.31.1)\n",
      "Requirement already satisfied: colorlog in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (4.0.2)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.9.151)\n",
      "Requirement already satisfied: pytorch-pretrained-bert==v0.6.0 in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.151 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 5)) (1.12.151)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.7/site-packages (from boto3->-r requirements.txt (line 5)) (0.9.4)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (2.21.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.151->boto3->-r requirements.txt (line 5)) (2.8.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20; python_version >= \"3.4\" in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.151->boto3->-r requirements.txt (line 5)) (1.24.1)\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.151->boto3->-r requirements.txt (line 5)) (0.14)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (2019.3.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.151->boto3->-r requirements.txt (line 5)) (1.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a folder /home/jupyter/mt-dnn/data\n",
      "mkdir: cannot create directory ‘/home/jupyter/mt-dnn/data’: File exists\n",
      "Create a folder BERT_DIR\n",
      "mkdir: cannot create directory ‘/home/jupyter/mt-dnn/mt_dnn_models’: File exists\n",
      "Cloning into 'jiant'...\n",
      "remote: Enumerating objects: 214, done.\u001b[K\n",
      "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
      "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
      "remote: Total 13408 (delta 142), reused 121 (delta 64), pack-reused 13194\u001b[K\n",
      "Receiving objects: 100% (13408/13408), 3.75 MiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (9899/9899), done.\n",
      "Downloading and extracting CoLA...\n",
      "\tCompleted!\n",
      "Downloading and extracting SST...\n",
      "\tCompleted!\n",
      "Processing MRPC...\n",
      "Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
      "\tCompleted!\n",
      "Downloading and extracting QQP...\n",
      "\tCompleted!\n",
      "Downloading and extracting STS...\n",
      "\tCompleted!\n",
      "Downloading and extracting MNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting SNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting QNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting RTE...\n",
      "\tCompleted!\n",
      "Downloading and extracting WNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting diagnostic data...\n",
      "\tCompleted!\n",
      "--2019-05-27 01:35:45--  http://data.allenai.org.s3.amazonaws.com/downloads/SciTailV1.1.zip\n",
      "Resolving data.allenai.org.s3.amazonaws.com (data.allenai.org.s3.amazonaws.com)... 52.218.245.146\n",
      "Connecting to data.allenai.org.s3.amazonaws.com (data.allenai.org.s3.amazonaws.com)|52.218.245.146|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14174621 (14M) [application/zip]\n",
      "Saving to: ‘SciTailV1.1.zip’\n",
      "\n",
      "SciTailV1.1.zip     100%[===================>]  13.52M  11.1MB/s    in 1.2s    \n",
      "\n",
      "2019-05-27 01:35:47 (11.1 MB/s) - ‘SciTailV1.1.zip’ saved [14174621/14174621]\n",
      "\n",
      "Archive:  SciTailV1.1.zip\n",
      "   creating: SciTailV1.1/\n",
      "   creating: SciTailV1.1/snli_format/\n",
      "  inflating: SciTailV1.1/snli_format/scitail_1.0_test.txt  \n",
      "  inflating: SciTailV1.1/snli_format/scitail_1.0_train.txt  \n",
      "  inflating: SciTailV1.1/snli_format/scitail_1.0_dev.txt  \n",
      "  inflating: SciTailV1.1/snli_format/README.txt  \n",
      "   creating: SciTailV1.1/dgem_format/\n",
      "  inflating: SciTailV1.1/dgem_format/scitail_1.0_structure_dev.tsv  \n",
      "  inflating: SciTailV1.1/dgem_format/scitail_1.0_structure_train.tsv  \n",
      "  inflating: SciTailV1.1/dgem_format/README.txt  \n",
      "  inflating: SciTailV1.1/dgem_format/scitail_1.0_structure_test.tsv  \n",
      "   creating: SciTailV1.1/predictor_format/\n",
      "  inflating: SciTailV1.1/predictor_format/scitail_1.0_structure_dev.jsonl  \n",
      "  inflating: SciTailV1.1/predictor_format/scitail_1.0_structure_test.jsonl  \n",
      "  inflating: SciTailV1.1/predictor_format/README.txt  \n",
      "  inflating: SciTailV1.1/predictor_format/scitail_1.0_structure_train.jsonl  \n",
      "  inflating: SciTailV1.1/all_annotations.tsv  \n",
      "  inflating: SciTailV1.1/README.txt  \n",
      "   creating: SciTailV1.1/tsv_format/\n",
      "  inflating: SciTailV1.1/tsv_format/scitail_1.0_test.tsv  \n",
      "  inflating: SciTailV1.1/tsv_format/scitail_1.0_train.tsv  \n",
      "  inflating: SciTailV1.1/tsv_format/scitail_1.0_dev.tsv  \n",
      "--2019-05-27 01:35:47--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 2607:f8b0:400e:c09::80\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 407727028 (389M) [application/zip]\n",
      "Saving to: ‘uncased_bert_base.zip’\n",
      "\n",
      "uncased_bert_base.z 100%[===================>] 388.84M  83.1MB/s    in 4.8s    \n",
      "\n",
      "2019-05-27 01:35:52 (80.8 MB/s) - ‘uncased_bert_base.zip’ saved [407727028/407727028]\n",
      "\n",
      "Archive:  uncased_bert_base.zip\n",
      "   creating: uncased_L-12_H-768_A-12/\n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n",
      "--2019-05-27 01:35:57--  https://mrc.blob.core.windows.net/mt-dnn-model/bert_model_base_v2.pt\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 437961819 (418M) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/mt-dnn/mt_dnn_models/bert_model_base.pt’\n",
      "\n",
      "/home/jupyter/mt-dn 100%[===================>] 417.67M  9.18MB/s    in 47s     \n",
      "\n",
      "2019-05-27 01:36:44 (8.97 MB/s) - ‘/home/jupyter/mt-dnn/mt_dnn_models/bert_model_base.pt’ saved [437961819/437961819]\n",
      "\n",
      "--2019-05-27 01:36:44--  https://mrc.blob.core.windows.net/mt-dnn-model/bert_model_large_v2.pt\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1340632883 (1.2G) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/mt-dnn/mt_dnn_models/bert_model_large.pt’\n",
      "\n",
      "/home/jupyter/mt-dn 100%[===================>]   1.25G  4.76MB/s    in 2m 45s  \n",
      "\n",
      "2019-05-27 01:39:30 (7.74 MB/s) - ‘/home/jupyter/mt-dnn/mt_dnn_models/bert_model_large.pt’ saved [1340632883/1340632883]\n",
      "\n",
      "--2019-05-27 01:39:30--  https://mrc.blob.core.windows.net/mt-dnn-model/mt_dnn_base.pt\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 437962060 (418M) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/mt-dnn/mt_dnn_models/mt_dnn_base.pt’\n",
      "\n",
      "/home/jupyter/mt-dn 100%[===================>] 417.67M  2.14MB/s    in 86s     \n",
      "\n",
      "2019-05-27 01:40:56 (4.88 MB/s) - ‘/home/jupyter/mt-dnn/mt_dnn_models/mt_dnn_base.pt’ saved [437962060/437962060]\n",
      "\n",
      "--2019-05-27 01:40:56--  https://mrc.blob.core.windows.net/mt-dnn-model/mt_dnn_large.pt\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1340632883 (1.2G) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/mt-dnn/mt_dnn_models/mt_dnn_large.pt’\n",
      "\n",
      "/home/jupyter/mt-dn 100%[===================>]   1.25G  10.2MB/s    in 2m 33s  \n",
      "\n",
      "2019-05-27 01:43:30 (8.37 MB/s) - ‘/home/jupyter/mt-dnn/mt_dnn_models/mt_dnn_large.pt’ saved [1340632883/1340632883]\n",
      "\n",
      "Create a folder /home/jupyter/mt-dnn/data\n",
      "mkdir: cannot create directory ‘domain_adaptation’: File exists\n",
      "--2019-05-27 01:43:30--  https://mrc.blob.core.windows.net/mt-dnn-model/data.zip\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 57236306 (55M) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>]  54.58M  12.6MB/s    in 3.7s    \n",
      "\n",
      "2019-05-27 01:43:34 (14.6 MB/s) - ‘data.zip’ saved [57236306/57236306]\n",
      "\n",
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/scitail_001_train.json  \n",
      "  inflating: data/scitail_01_train.json  \n",
      "  inflating: data/scitail_1_train.json  \n",
      "  inflating: data/scitail_5_train.json  \n",
      "  inflating: data/scitail_dev.json   \n",
      "  inflating: data/scitail_test.json  \n",
      "  inflating: data/scitail_train.json  \n",
      "  inflating: data/scitail_train_shuff.json  \n",
      "  inflating: data/snli_001_train.json  \n",
      "  inflating: data/snli_01_train.json  \n",
      "  inflating: data/snli_1_train.json  \n",
      "  inflating: data/snli_5_train.json  \n",
      "  inflating: data/snli_dev.json      \n",
      "  inflating: data/snli_test.json     \n",
      "  inflating: data/snli_train.json    \n",
      "  inflating: data/snli_train_shuff.json  \n"
     ]
    }
   ],
   "source": [
    "# !sh download.sh\n",
    "# note: The wrong QNLI dataset is downloaded right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "05/27/2019 04:05:13 Loaded 23596 SciTail train samples\n",
      "05/27/2019 04:05:13 Loaded 1304 SciTail dev samples\n",
      "05/27/2019 04:05:13 Loaded 2126 SciTail test samples\n",
      "05/27/2019 04:05:15 Loaded 549367 SNLI train samples\n",
      "05/27/2019 04:05:15 Loaded 9842 SNLI dev samples\n",
      "05/27/2019 04:05:15 Loaded 9824 SNLI test samples\n",
      "05/27/2019 04:05:17 Loaded 392702 MNLI train samples\n",
      "05/27/2019 04:05:17 Loaded 9815 MNLI matched dev samples\n",
      "05/27/2019 04:05:17 Loaded 9832 MNLI mismatched dev samples\n",
      "05/27/2019 04:05:17 Loaded 9796 MNLI matched test samples\n",
      "05/27/2019 04:05:17 Loaded 9847 MNLI mismatched test samples\n",
      "05/27/2019 04:05:17 Loaded 3668 MRPC train samples\n",
      "05/27/2019 04:05:17 Loaded 408 MRPC dev samples\n",
      "05/27/2019 04:05:17 Loaded 1725 MRPC test samples\n",
      "05/27/2019 04:05:17 Loaded 104743 QNLI train samples\n",
      "05/27/2019 04:05:17 Loaded 5463 QNLI dev samples\n",
      "05/27/2019 04:05:17 Loaded 5463 QNLI test samples\n",
      "05/27/2019 04:05:17 Loaded 104743 QNLI train samples\n",
      "05/27/2019 04:05:17 Loaded 5463 QNLI dev samples\n",
      "05/27/2019 04:05:17 Loaded 5463 QNLI test samples\n",
      "05/27/2019 04:05:19 Loaded 363849 QQP train samples\n",
      "05/27/2019 04:05:19 Loaded 40430 QQP dev samples\n",
      "05/27/2019 04:05:19 Loaded 390965 QQP test samples\n",
      "05/27/2019 04:05:19 Loaded 2490 RTE train samples\n",
      "05/27/2019 04:05:19 Loaded 277 RTE dev samples\n",
      "05/27/2019 04:05:19 Loaded 3000 RTE test samples\n",
      "05/27/2019 04:05:19 Loaded 635 WNLI train samples\n",
      "05/27/2019 04:05:19 Loaded 71 WNLI dev samples\n",
      "05/27/2019 04:05:19 Loaded 146 WNLI test samples\n",
      "05/27/2019 04:05:19 Loaded 67349 SST train samples\n",
      "05/27/2019 04:05:19 Loaded 872 SST dev samples\n",
      "05/27/2019 04:05:19 Loaded 1821 SST test samples\n",
      "05/27/2019 04:05:19 Loaded 5749 STS-B train samples\n",
      "05/27/2019 04:05:19 Loaded 1500 STS-B dev samples\n",
      "05/27/2019 04:05:19 Loaded 1379 STS-B test samples\n",
      "05/27/2019 04:05:19 Loaded 8550 COLA train samples\n",
      "05/27/2019 04:05:19 Loaded 1042 COLA dev samples\n",
      "05/27/2019 04:05:19 Loaded 1063 COLA test samples\n",
      "05/27/2019 04:05:34 done with scitail\n",
      "05/27/2019 04:08:43 done with snli\n",
      "05/27/2019 04:12:32 done with mnli\n",
      "05/27/2019 04:12:37 done with mrpc\n",
      "05/27/2019 04:13:56 done with qnli\n",
      "05/27/2019 04:13:56 done with qnli\n",
      "05/27/2019 04:19:08 done with qqp\n",
      "05/27/2019 04:19:13 done with rte\n",
      "05/27/2019 04:19:13 done with wnli\n",
      "05/27/2019 04:19:17 done with stsb\n",
      "05/27/2019 04:19:29 done with sst\n",
      "05/27/2019 04:19:31 done with cola\n"
     ]
    }
   ],
   "source": [
    "!python prepro.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Namespace(answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='data/mt_dnn', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, epochs=5, freeze_layers=-1, global_grad_clipping=1.0, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='mt_dnn/bert_model_base.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoint', pw_tasks=['qnnli'], ratio=0, scheduler_type='ms', seed=2018, task_config_path='configs/tasks_config.json', test_datasets=['mnli_mismatched', 'mnli_matched'], train_datasets=['mnli'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)\n",
      "05/27/2019 04:27:14 0\n",
      "05/27/2019 04:27:14 Launching the MT-DNN training\n",
      "05/27/2019 04:27:14 Loading data/mt_dnn/mnli_train.json as task 0\n",
      "Loaded 392702 samples out of 392702\n",
      "05/27/2019 04:27:22 3\n",
      "Loaded 9832 samples out of 9832\n",
      "Loaded 9847 samples out of 9847\n",
      "Loaded 9815 samples out of 9815\n",
      "Loaded 9796 samples out of 9796\n",
      "05/27/2019 04:27:24 ####################\n",
      "05/27/2019 04:27:24 {'log_file': 'mt-dnn-train.log', 'init_checkpoint': 'mt_dnn/bert_model_base.pt', 'data_dir': 'data/mt_dnn', 'data_sort_on': False, 'name': 'farmer', 'train_datasets': ['mnli'], 'test_datasets': ['mnli_mismatched', 'mnli_matched'], 'pw_tasks': ['qnnli'], 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': [0], 'label_size': '3', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'cuda': True, 'log_per_updates': 500, 'epochs': 5, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'ema_opt': 0, 'ema_gamma': 0.995, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': 'checkpoint', 'seed': 2018, 'task_config_path': 'configs/tasks_config.json', 'tasks_dropout_p': [0.1]}\n",
      "05/27/2019 04:27:24 ####################\n",
      "05/27/2019 04:27:24 ####################\n",
      "05/27/2019 04:27:24 Could not find the init model!\n",
      " The parameters will be initialized randomly!\n",
      "05/27/2019 04:27:24 ####################\n",
      "05/27/2019 04:27:27 \n",
      "############# Model Arch of MT-DNN #############\n",
      "SANBertNetwork(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (scoring_list): ModuleList(\n",
      "    (0): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "05/27/2019 04:27:27 Total number of params: 109484547\n",
      "05/27/2019 04:27:50 At epoch 0\n",
      "05/27/2019 04:27:51 Task [ 0] updates[     1] train loss[1.18678] remaining[12:59:57]\n",
      "05/27/2019 04:29:05 Task [ 0] updates[   500] train loss[1.10738] remaining[2:00:58]\n",
      "05/27/2019 04:30:19 Task [ 0] updates[  1000] train loss[1.10793] remaining[1:58:42]\n",
      "05/27/2019 04:31:32 Task [ 0] updates[  1500] train loss[1.10794] remaining[1:57:17]\n",
      "05/27/2019 04:32:46 Task [ 0] updates[  2000] train loss[1.10668] remaining[1:55:47]\n",
      "05/27/2019 04:33:59 Task [ 0] updates[  2500] train loss[1.10659] remaining[1:54:27]\n",
      "05/27/2019 04:35:12 Task [ 0] updates[  3000] train loss[1.10612] remaining[1:53:07]\n",
      "05/27/2019 04:36:25 Task [ 0] updates[  3500] train loss[1.10597] remaining[1:51:44]\n",
      "05/27/2019 04:37:39 Task [ 0] updates[  4000] train loss[1.10480] remaining[1:50:30]\n",
      "05/27/2019 04:38:52 Task [ 0] updates[  4500] train loss[1.10393] remaining[1:49:16]\n",
      "05/27/2019 04:40:05 Task [ 0] updates[  5000] train loss[1.10319] remaining[1:48:00]\n",
      "05/27/2019 04:41:18 Task [ 0] updates[  5500] train loss[1.10221] remaining[1:46:40]\n",
      "05/27/2019 04:42:31 Task [ 0] updates[  6000] train loss[1.10191] remaining[1:45:23]\n",
      "05/27/2019 04:43:44 Task [ 0] updates[  6500] train loss[1.10152] remaining[1:44:09]\n",
      "05/27/2019 04:44:58 Task [ 0] updates[  7000] train loss[1.10078] remaining[1:42:59]\n",
      "05/27/2019 04:46:12 Task [ 0] updates[  7500] train loss[1.10003] remaining[1:41:45]\n",
      "05/27/2019 04:47:24 Task [ 0] updates[  8000] train loss[1.09842] remaining[1:40:28]\n",
      "05/27/2019 04:48:36 Task [ 0] updates[  8500] train loss[1.09607] remaining[1:39:09]\n",
      "05/27/2019 04:49:49 Task [ 0] updates[  9000] train loss[1.09282] remaining[1:37:53]\n",
      "05/27/2019 04:51:03 Task [ 0] updates[  9500] train loss[1.08962] remaining[1:36:41]\n",
      "05/27/2019 04:52:15 Task [ 0] updates[ 10000] train loss[1.08652] remaining[1:35:25]\n",
      "05/27/2019 04:53:29 Task [ 0] updates[ 10500] train loss[1.08329] remaining[1:34:13]\n",
      "05/27/2019 04:54:42 Task [ 0] updates[ 11000] train loss[1.08063] remaining[1:33:00]\n",
      "05/27/2019 04:55:56 Task [ 0] updates[ 11500] train loss[1.07847] remaining[1:31:48]\n",
      "05/27/2019 04:57:10 Task [ 0] updates[ 12000] train loss[1.07664] remaining[1:30:37]\n",
      "05/27/2019 04:58:22 Task [ 0] updates[ 12500] train loss[1.07401] remaining[1:29:20]\n",
      "05/27/2019 04:59:35 Task [ 0] updates[ 13000] train loss[1.07169] remaining[1:28:07]\n",
      "05/27/2019 05:00:49 Task [ 0] updates[ 13500] train loss[1.07023] remaining[1:26:56]\n",
      "05/27/2019 05:02:03 Task [ 0] updates[ 14000] train loss[1.06837] remaining[1:25:43]\n",
      "05/27/2019 05:03:17 Task [ 0] updates[ 14500] train loss[1.06667] remaining[1:24:33]\n",
      "05/27/2019 05:04:31 Task [ 0] updates[ 15000] train loss[1.06503] remaining[1:23:20]\n",
      "05/27/2019 05:05:45 Task [ 0] updates[ 15500] train loss[1.06344] remaining[1:22:08]\n",
      "05/27/2019 05:06:59 Task [ 0] updates[ 16000] train loss[1.06222] remaining[1:20:56]\n",
      "05/27/2019 05:08:12 Task [ 0] updates[ 16500] train loss[1.06087] remaining[1:19:42]\n",
      "05/27/2019 05:09:25 Task [ 0] updates[ 17000] train loss[1.05908] remaining[1:18:28]\n",
      "05/27/2019 05:10:39 Task [ 0] updates[ 17500] train loss[1.05766] remaining[1:17:16]\n",
      "05/27/2019 05:11:53 Task [ 0] updates[ 18000] train loss[1.05655] remaining[1:16:03]\n",
      "05/27/2019 05:13:06 Task [ 0] updates[ 18500] train loss[1.05525] remaining[1:14:49]\n",
      "05/27/2019 05:14:18 Task [ 0] updates[ 19000] train loss[1.05404] remaining[1:13:34]\n",
      "05/27/2019 05:15:31 Task [ 0] updates[ 19500] train loss[1.05278] remaining[1:12:20]\n",
      "05/27/2019 05:16:43 Task [ 0] updates[ 20000] train loss[1.05143] remaining[1:11:04]\n",
      "05/27/2019 05:17:56 Task [ 0] updates[ 20500] train loss[1.05035] remaining[1:09:50]\n",
      "05/27/2019 05:19:08 Task [ 0] updates[ 21000] train loss[1.04934] remaining[1:08:35]\n",
      "05/27/2019 05:20:21 Task [ 0] updates[ 21500] train loss[1.04820] remaining[1:07:22]\n",
      "05/27/2019 05:21:34 Task [ 0] updates[ 22000] train loss[1.04711] remaining[1:06:08]\n",
      "05/27/2019 05:22:46 Task [ 0] updates[ 22500] train loss[1.04619] remaining[1:04:54]\n",
      "05/27/2019 05:23:57 Task [ 0] updates[ 23000] train loss[1.04490] remaining[1:03:38]\n",
      "05/27/2019 05:25:10 Task [ 0] updates[ 23500] train loss[1.04378] remaining[1:02:25]\n",
      "05/27/2019 05:26:23 Task [ 0] updates[ 24000] train loss[1.04267] remaining[1:01:12]\n",
      "05/27/2019 05:27:36 Task [ 0] updates[ 24500] train loss[1.04173] remaining[0:59:58]\n",
      "05/27/2019 05:28:48 Task [ 0] updates[ 25000] train loss[1.04074] remaining[0:58:43]\n",
      "05/27/2019 05:30:00 Task [ 0] updates[ 25500] train loss[1.03976] remaining[0:57:30]\n",
      "05/27/2019 05:31:13 Task [ 0] updates[ 26000] train loss[1.03875] remaining[0:56:16]\n",
      "05/27/2019 05:32:25 Task [ 0] updates[ 26500] train loss[1.03783] remaining[0:55:02]\n",
      "05/27/2019 05:33:37 Task [ 0] updates[ 27000] train loss[1.03701] remaining[0:53:48]\n",
      "05/27/2019 05:34:48 Task [ 0] updates[ 27500] train loss[1.03623] remaining[0:52:34]\n",
      "05/27/2019 05:36:00 Task [ 0] updates[ 28000] train loss[1.03512] remaining[0:51:19]\n",
      "05/27/2019 05:37:12 Task [ 0] updates[ 28500] train loss[1.03415] remaining[0:50:06]\n",
      "05/27/2019 05:38:25 Task [ 0] updates[ 29000] train loss[1.03322] remaining[0:48:53]\n",
      "05/27/2019 05:39:37 Task [ 0] updates[ 29500] train loss[1.03230] remaining[0:47:39]\n",
      "05/27/2019 05:40:50 Task [ 0] updates[ 30000] train loss[1.03130] remaining[0:46:26]\n",
      "05/27/2019 05:42:02 Task [ 0] updates[ 30500] train loss[1.03027] remaining[0:45:13]\n",
      "05/27/2019 05:43:15 Task [ 0] updates[ 31000] train loss[1.02930] remaining[0:44:00]\n",
      "05/27/2019 05:44:27 Task [ 0] updates[ 31500] train loss[1.02820] remaining[0:42:46]\n",
      "05/27/2019 05:45:40 Task [ 0] updates[ 32000] train loss[1.02738] remaining[0:41:33]\n",
      "05/27/2019 05:46:52 Task [ 0] updates[ 32500] train loss[1.02657] remaining[0:40:20]\n",
      "05/27/2019 05:48:04 Task [ 0] updates[ 33000] train loss[1.02572] remaining[0:39:06]\n",
      "05/27/2019 05:49:16 Task [ 0] updates[ 33500] train loss[1.02467] remaining[0:37:53]\n",
      "05/27/2019 05:50:27 Task [ 0] updates[ 34000] train loss[1.02396] remaining[0:36:39]\n",
      "05/27/2019 05:51:40 Task [ 0] updates[ 34500] train loss[1.02310] remaining[0:35:26]\n",
      "05/27/2019 05:52:53 Task [ 0] updates[ 35000] train loss[1.02230] remaining[0:34:13]\n",
      "05/27/2019 05:54:05 Task [ 0] updates[ 35500] train loss[1.02188] remaining[0:33:00]\n",
      "05/27/2019 05:55:16 Task [ 0] updates[ 36000] train loss[1.02114] remaining[0:31:47]\n",
      "05/27/2019 05:56:29 Task [ 0] updates[ 36500] train loss[1.02051] remaining[0:30:34]\n",
      "05/27/2019 05:57:42 Task [ 0] updates[ 37000] train loss[1.01962] remaining[0:29:21]\n",
      "05/27/2019 05:58:54 Task [ 0] updates[ 37500] train loss[1.01897] remaining[0:28:08]\n",
      "05/27/2019 06:00:06 Task [ 0] updates[ 38000] train loss[1.01814] remaining[0:26:55]\n",
      "05/27/2019 06:01:19 Task [ 0] updates[ 38500] train loss[1.01731] remaining[0:25:42]\n",
      "05/27/2019 06:02:31 Task [ 0] updates[ 39000] train loss[1.01651] remaining[0:24:29]\n",
      "05/27/2019 06:03:45 Task [ 0] updates[ 39500] train loss[1.01578] remaining[0:23:16]\n",
      "05/27/2019 06:04:57 Task [ 0] updates[ 40000] train loss[1.01515] remaining[0:22:03]\n",
      "05/27/2019 06:06:10 Task [ 0] updates[ 40500] train loss[1.01429] remaining[0:20:50]\n",
      "05/27/2019 06:07:23 Task [ 0] updates[ 41000] train loss[1.01351] remaining[0:19:38]\n",
      "05/27/2019 06:08:37 Task [ 0] updates[ 41500] train loss[1.01268] remaining[0:18:25]\n",
      "05/27/2019 06:09:50 Task [ 0] updates[ 42000] train loss[1.01218] remaining[0:17:12]\n",
      "05/27/2019 06:11:04 Task [ 0] updates[ 42500] train loss[1.01143] remaining[0:16:00]\n",
      "05/27/2019 06:12:17 Task [ 0] updates[ 43000] train loss[1.01082] remaining[0:14:47]\n",
      "05/27/2019 06:13:31 Task [ 0] updates[ 43500] train loss[1.01019] remaining[0:13:34]\n",
      "05/27/2019 06:14:44 Task [ 0] updates[ 44000] train loss[1.00965] remaining[0:12:21]\n",
      "05/27/2019 06:15:58 Task [ 0] updates[ 44500] train loss[1.00902] remaining[0:11:08]\n",
      "05/27/2019 06:17:11 Task [ 0] updates[ 45000] train loss[1.00854] remaining[0:09:55]\n",
      "05/27/2019 06:18:24 Task [ 0] updates[ 45500] train loss[1.00782] remaining[0:08:43]\n",
      "05/27/2019 06:19:39 Task [ 0] updates[ 46000] train loss[1.00713] remaining[0:07:30]\n",
      "05/27/2019 06:20:52 Task [ 0] updates[ 46500] train loss[1.00650] remaining[0:06:17]\n",
      "05/27/2019 06:22:06 Task [ 0] updates[ 47000] train loss[1.00577] remaining[0:05:04]\n",
      "05/27/2019 06:23:19 Task [ 0] updates[ 47500] train loss[1.00520] remaining[0:03:51]\n",
      "05/27/2019 06:24:33 Task [ 0] updates[ 48000] train loss[1.00448] remaining[0:02:38]\n",
      "05/27/2019 06:25:56 Task [ 0] updates[ 48500] train loss[1.00401] remaining[0:01:25]\n",
      "05/27/2019 06:27:10 Task [ 0] updates[ 49000] train loss[1.00354] remaining[0:00:12]\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 353, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 327, in main\n",
      "    use_cuda=args.cuda)\n",
      "  File \"/home/jupyter/mt-dnn/data_utils/glue_utils.py\", line 325, in eval_model\n",
      "    score, pred, gold = model.predict(batch_meta, batch_data)\n",
      "  File \"/home/jupyter/mt-dnn/mt_dnn/model.py\", line 180, in predict\n",
      "    score = score.numpy()\n",
      "RuntimeError: PyTorch was compiled without NumPy support\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripts/run_mt_dnn.sh: 2: scripts/run_mt_dnn.sh: [[: not found\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Namespace(answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=1, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=32, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='../data/mt_dnn', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, epochs=5, freeze_layers=-1, global_grad_clipping=1.0, grad_clipping=0.0, have_lr_scheduler=True, init_checkpoint='../mt_dnn_models/bert_model_large.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='checkpoints/mt-dnn-rte_adamax_answer_opt1_gc0_ggc1_2019-05-18T0717/log.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, momentum=0, mtl_opt=0, multi_gpu_on=True, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoints/mt-dnn-rte_adamax_answer_opt1_gc0_ggc1_2019-05-18T0717', pw_tasks=['qnnli'], ratio=0, scheduler_type='ms', seed=2018, task_config_path='configs/tasks_config.json', test_datasets=['mnli_matched', 'mnli_mismatched', 'rte'], train_datasets=['mnli', 'rte', 'qqp', 'qnli', 'mrpc', 'sst', 'cola', 'stsb'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)\n",
      "05/18/2019 07:17:19 1\n",
      "05/18/2019 07:17:19 Launching the MT-DNN training\n",
      "05/18/2019 07:17:19 Loading ../data/mt_dnn/mnli_train.json as task 0\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 353, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 181, in main\n",
      "    train_data = BatchGen(BatchGen.load(train_path, True, pairwise=pw_task, maxlen=args.max_seq_len),\n",
      "  File \"/home/jupyter/mt-dnn/mt_dnn/batcher.py\", line 52, in load\n",
      "    with open(path, 'r', encoding='utf-8') as reader:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/mt_dnn/mnli_train.json'\n"
     ]
    }
   ],
   "source": [
    "!sh scripts/run_mt_dnn.sh 32 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripts/run_stsb.sh: 2: scripts/run_stsb.sh: [[: not found\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Namespace(answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=32, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='../data/mt_dnn', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, epochs=5, freeze_layers=-1, global_grad_clipping=1.0, grad_clipping=0.0, have_lr_scheduler=True, init_checkpoint='../mt_dnn_models/mt_dnn_large.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='checkpoints/mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-27T0125/log.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoints/mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-27T0125', pw_tasks=['qnnli'], ratio=0, scheduler_type='ms', seed=2018, task_config_path='configs/tasks_config.json', test_datasets=['stsb'], train_datasets=['stsb'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)\n",
      "05/27/2019 01:25:58 0\n",
      "05/27/2019 01:25:58 Launching the MT-DNN training\n",
      "05/27/2019 01:25:58 Loading ../data/mt_dnn/stsb_train.json as task 0\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 353, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 181, in main\n",
      "    train_data = BatchGen(BatchGen.load(train_path, True, pairwise=pw_task, maxlen=args.max_seq_len),\n",
      "  File \"/home/jupyter/mt-dnn/mt_dnn/batcher.py\", line 52, in load\n",
      "    with open(path, 'r', encoding='utf-8') as reader:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/mt_dnn/stsb_train.json'\n"
     ]
    }
   ],
   "source": [
    "!sh scripts/run_stsb.sh 32 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
