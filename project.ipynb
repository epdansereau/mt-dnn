{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test GPU:\n",
    "# import torch\n",
    "# torch.cuda.current_device()\n",
    "# torch.cuda.device(0)\n",
    "# torch.cuda.device_count()\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mt-dnn\n"
     ]
    }
   ],
   "source": [
    "# should be in /home/jupyter/mt-dnn\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_data_proc_512.log\tdata_utils   module\t    project.ipynb     scripts\n",
      "checkpoints\t\tdocker\t     mt_dnn\t    README.md\t      TEST.txt\n",
      "config\t\t\tdownload.sh  mt_dnn_models  requirements.txt  train.py\n",
      "data\t\t\tLICENSE      prepro.py\t    run_toy.sh\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.0\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.16.2)\n",
      "Collecting torch==0.4.1.post2 (from -r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/91/1b2871d6c8ca079254deae5872af32e02e9a85f07dd0834e8b3489ce138f/torch-0.4.1.post2-cp37-cp37m-manylinux1_x86_64.whl (519.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 519.5MB 85kB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (4.31.1)\n",
      "Collecting colorlog (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/68/4d/892728b0c14547224f0ac40884e722a3d00cb54e7a146aea0b3186806c9e/colorlog-4.0.2-py2.py3-none-any.whl\n",
      "Collecting boto3 (from -r requirements.txt (line 5))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/b0/8e67d52f8787370828709fe4cad39184e65f3fec28023b78c2556a04be0c/boto3-1.9.158-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 28.5MB/s \n",
      "\u001b[?25hCollecting pytorch-pretrained-bert==v0.6.0 (from -r requirements.txt (line 6))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/ac/8d72155697620bb9b453dcde3ad8520dc1464fa3abde389afbd542c50402/pytorch_pretrained_bert-0.6.0-py3-none-any.whl (114kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 234kB/s \n",
      "\u001b[?25hCollecting s3transfer<0.3.0,>=0.2.0 (from boto3->-r requirements.txt (line 5))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/de/5737f602e22073ecbded7a0c590707085e154e32b68d86545dcc31004c02/s3transfer-0.2.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 27.3MB/s \n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting botocore<1.13.0,>=1.12.158 (from boto3->-r requirements.txt (line 5))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/21/1e6f12ef2642786bda935f9936bfe3a3f19466a1f3f7e079053421a39f0a/botocore-1.12.158-py2.py3-none-any.whl (5.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.5MB 8.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (2.21.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.158->boto3->-r requirements.txt (line 5)) (2.8.0)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.158->boto3->-r requirements.txt (line 5)) (1.24.1)\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.158->boto3->-r requirements.txt (line 5)) (0.14)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->pytorch-pretrained-bert==v0.6.0->-r requirements.txt (line 6)) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.158->boto3->-r requirements.txt (line 5)) (1.10.0)\n",
      "\u001b[31mfastai 1.0.52 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[31mtorchvision 0.3.0 has requirement torch>=1.1.0, but you'll have torch 0.4.1.post2 which is incompatible.\u001b[0m\n",
      "\u001b[31mfastai 1.0.52 has requirement torch>=1.0.0, but you'll have torch 0.4.1.post2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch, colorlog, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
      "  Found existing installation: torch 1.1.0\n",
      "    Uninstalling torch-1.1.0:\n",
      "      Successfully uninstalled torch-1.1.0\n",
      "Successfully installed boto3-1.9.158 botocore-1.12.158 colorlog-4.0.2 jmespath-0.9.4 pytorch-pretrained-bert-0.6.0 s3transfer-0.2.0 torch-0.4.1.post2\n"
     ]
    }
   ],
   "source": [
    "#I've changed requirements.txt to use pytorch 0.4.1.post2 instead of 0.4.1 \n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a folder /home/jupyter/mt-dnn/data\n",
      "Create a folder BERT_DIR\n",
      "Cloning into 'jiant'...\n",
      "remote: Enumerating objects: 219, done.\u001b[K\n",
      "remote: Counting objects: 100% (219/219), done.\u001b[K\n",
      "remote: Compressing objects: 100% (153/153), done.\u001b[K\n",
      "remote: Total 13413 (delta 144), reused 125 (delta 65), pack-reused 13194\u001b[K\n",
      "Receiving objects: 100% (13413/13413), 3.76 MiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (9897/9897), done.\n",
      "Downloading and extracting CoLA...\n",
      "\tCompleted!\n",
      "Downloading and extracting SST...\n",
      "\tCompleted!\n",
      "Processing MRPC...\n",
      "Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
      "\tCompleted!\n",
      "Downloading and extracting QQP...\n",
      "\tCompleted!\n",
      "Downloading and extracting STS...\n",
      "\tCompleted!\n",
      "Downloading and extracting MNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting SNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting QNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting RTE...\n",
      "\tCompleted!\n",
      "Downloading and extracting WNLI...\n",
      "\tCompleted!\n",
      "Downloading and extracting diagnostic data...\n",
      "\tCompleted!\n",
      "--2019-05-30 05:13:53--  http://data.allenai.org.s3.amazonaws.com/downloads/SciTailV1.1.zip\n",
      "Resolving data.allenai.org.s3.amazonaws.com (data.allenai.org.s3.amazonaws.com)... 54.231.184.242\n",
      "Connecting to data.allenai.org.s3.amazonaws.com (data.allenai.org.s3.amazonaws.com)|54.231.184.242|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14174621 (14M) [application/zip]\n",
      "Saving to: ‘SciTailV1.1.zip’\n",
      "\n",
      "SciTailV1.1.zip     100%[===================>]  13.52M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2019-05-30 05:13:53 (91.3 MB/s) - ‘SciTailV1.1.zip’ saved [14174621/14174621]\n",
      "\n",
      "Archive:  SciTailV1.1.zip\n",
      "   creating: SciTailV1.1/\n",
      "   creating: SciTailV1.1/snli_format/\n",
      "  inflating: SciTailV1.1/snli_format/scitail_1.0_test.txt  \n",
      "  inflating: SciTailV1.1/snli_format/scitail_1.0_train.txt  \n",
      "  inflating: SciTailV1.1/snli_format/scitail_1.0_dev.txt  \n",
      "  inflating: SciTailV1.1/snli_format/README.txt  \n",
      "   creating: SciTailV1.1/dgem_format/\n",
      "  inflating: SciTailV1.1/dgem_format/scitail_1.0_structure_dev.tsv  \n",
      "  inflating: SciTailV1.1/dgem_format/scitail_1.0_structure_train.tsv  \n",
      "  inflating: SciTailV1.1/dgem_format/README.txt  \n",
      "  inflating: SciTailV1.1/dgem_format/scitail_1.0_structure_test.tsv  \n",
      "   creating: SciTailV1.1/predictor_format/\n",
      "  inflating: SciTailV1.1/predictor_format/scitail_1.0_structure_dev.jsonl  \n",
      "  inflating: SciTailV1.1/predictor_format/scitail_1.0_structure_test.jsonl  \n",
      "  inflating: SciTailV1.1/predictor_format/README.txt  \n",
      "  inflating: SciTailV1.1/predictor_format/scitail_1.0_structure_train.jsonl  \n",
      "  inflating: SciTailV1.1/all_annotations.tsv  \n",
      "  inflating: SciTailV1.1/README.txt  \n",
      "   creating: SciTailV1.1/tsv_format/\n",
      "  inflating: SciTailV1.1/tsv_format/scitail_1.0_test.tsv  \n",
      "  inflating: SciTailV1.1/tsv_format/scitail_1.0_train.tsv  \n",
      "  inflating: SciTailV1.1/tsv_format/scitail_1.0_dev.tsv  \n",
      "--2019-05-30 05:13:54--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.195.128, 2607:f8b0:400e:c06::80\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.195.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 407727028 (389M) [application/zip]\n",
      "Saving to: ‘uncased_bert_base.zip’\n",
      "\n",
      "uncased_bert_base.z 100%[===================>] 388.84M   156MB/s    in 2.5s    \n",
      "\n",
      "2019-05-30 05:13:57 (156 MB/s) - ‘uncased_bert_base.zip’ saved [407727028/407727028]\n",
      "\n",
      "Archive:  uncased_bert_base.zip\n",
      "   creating: uncased_L-12_H-768_A-12/\n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n",
      "--2019-05-30 05:14:01--  https://mrc.blob.core.windows.net/mt-dnn-model/bert_model_base_v2.pt\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 437961819 (418M) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/mt-dnn/mt_dnn_models/bert_model_base.pt’\n",
      "\n",
      "/home/jupyter/mt-dn 100%[===================>] 417.67M  20.4MB/s    in 23s     \n",
      "\n",
      "2019-05-30 05:14:24 (18.6 MB/s) - ‘/home/jupyter/mt-dnn/mt_dnn_models/bert_model_base.pt’ saved [437961819/437961819]\n",
      "\n",
      "--2019-05-30 05:14:24--  https://mrc.blob.core.windows.net/mt-dnn-model/bert_model_large_v2.pt\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1340632883 (1.2G) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/mt-dnn/mt_dnn_models/bert_model_large.pt’\n",
      "\n",
      "/home/jupyter/mt-dn 100%[===================>]   1.25G  9.72MB/s    in 98s     \n",
      "\n",
      "2019-05-30 05:16:02 (13.1 MB/s) - ‘/home/jupyter/mt-dnn/mt_dnn_models/bert_model_large.pt’ saved [1340632883/1340632883]\n",
      "\n",
      "--2019-05-30 05:16:02--  https://mrc.blob.core.windows.net/mt-dnn-model/mt_dnn_base.pt\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 437962060 (418M) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/mt-dnn/mt_dnn_models/mt_dnn_base.pt’\n",
      "\n",
      "/home/jupyter/mt-dn 100%[===================>] 417.67M  8.87MB/s    in 25s     \n",
      "\n",
      "2019-05-30 05:16:28 (16.4 MB/s) - ‘/home/jupyter/mt-dnn/mt_dnn_models/mt_dnn_base.pt’ saved [437962060/437962060]\n",
      "\n",
      "--2019-05-30 05:16:28--  https://mrc.blob.core.windows.net/mt-dnn-model/mt_dnn_large.pt\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1340632883 (1.2G) [application/octet-stream]\n",
      "Saving to: ‘/home/jupyter/mt-dnn/mt_dnn_models/mt_dnn_large.pt’\n",
      "\n",
      "/home/jupyter/mt-dn 100%[===================>]   1.25G  24.8MB/s    in 1m 51s  \n",
      "\n",
      "2019-05-30 05:18:20 (11.5 MB/s) - ‘/home/jupyter/mt-dnn/mt_dnn_models/mt_dnn_large.pt’ saved [1340632883/1340632883]\n",
      "\n",
      "Create a folder /home/jupyter/mt-dnn/data\n",
      "--2019-05-30 05:18:20--  https://mrc.blob.core.windows.net/mt-dnn-model/data.zip\n",
      "Resolving mrc.blob.core.windows.net (mrc.blob.core.windows.net)... 52.190.240.132\n",
      "Connecting to mrc.blob.core.windows.net (mrc.blob.core.windows.net)|52.190.240.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 57236306 (55M) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>]  54.58M  48.3MB/s    in 1.1s    \n",
      "\n",
      "2019-05-30 05:18:22 (48.3 MB/s) - ‘data.zip’ saved [57236306/57236306]\n",
      "\n",
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/scitail_001_train.json  \n",
      "  inflating: data/scitail_01_train.json  \n",
      "  inflating: data/scitail_1_train.json  \n",
      "  inflating: data/scitail_5_train.json  \n",
      "  inflating: data/scitail_dev.json   \n",
      "  inflating: data/scitail_test.json  \n",
      "  inflating: data/scitail_train.json  \n",
      "  inflating: data/scitail_train_shuff.json  \n",
      "  inflating: data/snli_001_train.json  \n",
      "  inflating: data/snli_01_train.json  \n",
      "  inflating: data/snli_1_train.json  \n",
      "  inflating: data/snli_5_train.json  \n",
      "  inflating: data/snli_dev.json      \n",
      "  inflating: data/snli_test.json     \n",
      "  inflating: data/snli_train.json    \n",
      "  inflating: data/snli_train_shuff.json  \n"
     ]
    }
   ],
   "source": [
    "!sh download.sh\n",
    "# note: The wrong QNLI dataset is downloaded right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "100%|███████████████████████████████| 231508/231508 [00:00<00:00, 1132781.84B/s]\n",
      "05/30/2019 05:20:13 Loaded 23596 SciTail train samples\n",
      "05/30/2019 05:20:13 Loaded 1304 SciTail dev samples\n",
      "05/30/2019 05:20:13 Loaded 2126 SciTail test samples\n",
      "05/30/2019 05:20:15 Loaded 549367 SNLI train samples\n",
      "05/30/2019 05:20:15 Loaded 9842 SNLI dev samples\n",
      "05/30/2019 05:20:15 Loaded 9824 SNLI test samples\n",
      "05/30/2019 05:20:17 Loaded 392702 MNLI train samples\n",
      "05/30/2019 05:20:17 Loaded 9815 MNLI matched dev samples\n",
      "05/30/2019 05:20:17 Loaded 9832 MNLI mismatched dev samples\n",
      "05/30/2019 05:20:17 Loaded 9796 MNLI matched test samples\n",
      "05/30/2019 05:20:17 Loaded 9847 MNLI mismatched test samples\n",
      "05/30/2019 05:20:17 Loaded 3668 MRPC train samples\n",
      "05/30/2019 05:20:17 Loaded 408 MRPC dev samples\n",
      "05/30/2019 05:20:17 Loaded 1725 MRPC test samples\n",
      "05/30/2019 05:20:17 Loaded 104743 QNLI train samples\n",
      "05/30/2019 05:20:17 Loaded 5463 QNLI dev samples\n",
      "05/30/2019 05:20:17 Loaded 5463 QNLI test samples\n",
      "05/30/2019 05:20:17 Loaded 104743 QNLI train samples\n",
      "05/30/2019 05:20:17 Loaded 5463 QNLI dev samples\n",
      "05/30/2019 05:20:17 Loaded 5463 QNLI test samples\n",
      "05/30/2019 05:20:19 Loaded 363849 QQP train samples\n",
      "05/30/2019 05:20:19 Loaded 40430 QQP dev samples\n",
      "05/30/2019 05:20:19 Loaded 390965 QQP test samples\n",
      "05/30/2019 05:20:19 Loaded 2490 RTE train samples\n",
      "05/30/2019 05:20:19 Loaded 277 RTE dev samples\n",
      "05/30/2019 05:20:19 Loaded 3000 RTE test samples\n",
      "05/30/2019 05:20:19 Loaded 635 WNLI train samples\n",
      "05/30/2019 05:20:19 Loaded 71 WNLI dev samples\n",
      "05/30/2019 05:20:19 Loaded 146 WNLI test samples\n",
      "05/30/2019 05:20:19 Loaded 67349 SST train samples\n",
      "05/30/2019 05:20:19 Loaded 872 SST dev samples\n",
      "05/30/2019 05:20:19 Loaded 1821 SST test samples\n",
      "05/30/2019 05:20:19 Loaded 5749 STS-B train samples\n",
      "05/30/2019 05:20:19 Loaded 1500 STS-B dev samples\n",
      "05/30/2019 05:20:19 Loaded 1379 STS-B test samples\n",
      "05/30/2019 05:20:19 Loaded 8550 COLA train samples\n",
      "05/30/2019 05:20:19 Loaded 1042 COLA dev samples\n",
      "05/30/2019 05:20:19 Loaded 1063 COLA test samples\n",
      "05/30/2019 05:20:33 done with scitail\n",
      "05/30/2019 05:23:39 done with snli\n",
      "05/30/2019 05:27:28 done with mnli\n",
      "05/30/2019 05:27:32 done with mrpc\n",
      "05/30/2019 05:28:50 done with qnli\n",
      "05/30/2019 05:28:50 done with qnli\n",
      "05/30/2019 05:33:56 done with qqp\n",
      "05/30/2019 05:34:01 done with rte\n",
      "05/30/2019 05:34:01 done with wnli\n",
      "05/30/2019 05:34:05 done with stsb\n",
      "05/30/2019 05:34:17 done with sst\n",
      "05/30/2019 05:34:19 done with cola\n"
     ]
    }
   ],
   "source": [
    "!python prepro.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Namespace(answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='data/mt_dnn', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, epochs=5, freeze_layers=-1, global_grad_clipping=1.0, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='mt_dnn_models/mt_dnn_base.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoint', pw_tasks=['qnnli'], ratio=0, scheduler_type='ms', seed=2018, task_config_path='configs/tasks_config.json', test_datasets=['mnli_mismatched', 'mnli_matched'], train_datasets=['mnli'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)\n",
      "05/27/2019 10:35:24 0\n",
      "05/27/2019 10:35:24 Launching the MT-DNN training\n",
      "05/27/2019 10:35:24 Loading data/mt_dnn/mnli_train.json as task 0\n",
      "Loaded 392702 samples out of 392702\n",
      "05/27/2019 10:35:32 3\n",
      "Loaded 9832 samples out of 9832\n",
      "Loaded 9847 samples out of 9847\n",
      "Loaded 9815 samples out of 9815\n",
      "Loaded 9796 samples out of 9796\n",
      "05/27/2019 10:35:35 ####################\n",
      "05/27/2019 10:35:35 {'log_file': 'mt-dnn-train.log', 'init_checkpoint': 'mt_dnn_models/mt_dnn_base.pt', 'data_dir': 'data/mt_dnn', 'data_sort_on': False, 'name': 'farmer', 'train_datasets': ['mnli'], 'test_datasets': ['mnli_mismatched', 'mnli_matched'], 'pw_tasks': ['qnnli'], 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': [0], 'label_size': '3', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'cuda': True, 'log_per_updates': 500, 'epochs': 5, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'ema_opt': 0, 'ema_gamma': 0.995, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': 'checkpoint', 'seed': 2018, 'task_config_path': 'configs/tasks_config.json', 'tasks_dropout_p': [0.1]}\n",
      "05/27/2019 10:35:35 ####################\n",
      "05/27/2019 10:35:38 \n",
      "############# Model Arch of MT-DNN #############\n",
      "SANBertNetwork(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (scoring_list): ModuleList(\n",
      "    (0): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "05/27/2019 10:35:38 Total number of params: 109484547\n",
      "05/27/2019 10:35:41 At epoch 0\n",
      "05/27/2019 10:35:41 Task [ 0] updates[     1] train loss[0.90238] remaining[3:55:16]\n",
      "05/27/2019 10:36:59 Task [ 0] updates[   500] train loss[0.98695] remaining[2:05:30]\n",
      "05/27/2019 10:38:16 Task [ 0] updates[  1000] train loss[0.84438] remaining[2:03:56]\n",
      "05/27/2019 10:39:34 Task [ 0] updates[  1500] train loss[0.70324] remaining[2:02:55]\n",
      "05/27/2019 10:40:51 Task [ 0] updates[  2000] train loss[0.59478] remaining[2:01:31]\n",
      "05/27/2019 10:42:08 Task [ 0] updates[  2500] train loss[0.52244] remaining[2:00:10]\n",
      "05/27/2019 10:43:25 Task [ 0] updates[  3000] train loss[0.47121] remaining[1:58:52]\n",
      "05/27/2019 10:44:42 Task [ 0] updates[  3500] train loss[0.43500] remaining[1:57:29]\n",
      "05/27/2019 10:46:00 Task [ 0] updates[  4000] train loss[0.40730] remaining[1:56:14]\n",
      "05/27/2019 10:47:17 Task [ 0] updates[  4500] train loss[0.38723] remaining[1:54:53]\n",
      "05/27/2019 10:48:34 Task [ 0] updates[  5000] train loss[0.37212] remaining[1:53:37]\n",
      "05/27/2019 10:49:52 Task [ 0] updates[  5500] train loss[0.35901] remaining[1:52:19]\n",
      "05/27/2019 10:51:08 Task [ 0] updates[  6000] train loss[0.34976] remaining[1:50:59]\n",
      "05/27/2019 10:52:26 Task [ 0] updates[  6500] train loss[0.34194] remaining[1:49:44]\n",
      "05/27/2019 10:53:44 Task [ 0] updates[  7000] train loss[0.33334] remaining[1:48:29]\n",
      "05/27/2019 10:55:00 Task [ 0] updates[  7500] train loss[0.32896] remaining[1:47:08]\n",
      "05/27/2019 10:56:17 Task [ 0] updates[  8000] train loss[0.32384] remaining[1:45:48]\n",
      "05/27/2019 10:57:34 Task [ 0] updates[  8500] train loss[0.31898] remaining[1:44:28]\n",
      "05/27/2019 10:58:51 Task [ 0] updates[  9000] train loss[0.31543] remaining[1:43:09]\n",
      "05/27/2019 11:00:08 Task [ 0] updates[  9500] train loss[0.31340] remaining[1:41:52]\n",
      "05/27/2019 11:01:25 Task [ 0] updates[ 10000] train loss[0.31056] remaining[1:40:33]\n",
      "05/27/2019 11:02:42 Task [ 0] updates[ 10500] train loss[0.30878] remaining[1:39:16]\n",
      "05/27/2019 11:03:59 Task [ 0] updates[ 11000] train loss[0.30745] remaining[1:37:58]\n",
      "05/27/2019 11:05:16 Task [ 0] updates[ 11500] train loss[0.30595] remaining[1:36:42]\n",
      "05/27/2019 11:06:34 Task [ 0] updates[ 12000] train loss[0.30532] remaining[1:35:27]\n",
      "05/27/2019 11:07:51 Task [ 0] updates[ 12500] train loss[0.30435] remaining[1:34:08]\n",
      "05/27/2019 11:09:07 Task [ 0] updates[ 13000] train loss[0.30256] remaining[1:32:49]\n",
      "05/27/2019 11:10:25 Task [ 0] updates[ 13500] train loss[0.30221] remaining[1:31:33]\n",
      "05/27/2019 11:11:42 Task [ 0] updates[ 14000] train loss[0.30127] remaining[1:30:15]\n",
      "05/27/2019 11:13:00 Task [ 0] updates[ 14500] train loss[0.30085] remaining[1:29:00]\n",
      "05/27/2019 11:14:17 Task [ 0] updates[ 15000] train loss[0.30072] remaining[1:27:41]\n",
      "05/27/2019 11:15:34 Task [ 0] updates[ 15500] train loss[0.30067] remaining[1:26:24]\n",
      "05/27/2019 11:16:51 Task [ 0] updates[ 16000] train loss[0.30016] remaining[1:25:07]\n",
      "05/27/2019 11:18:08 Task [ 0] updates[ 16500] train loss[0.30013] remaining[1:23:50]\n",
      "05/27/2019 11:19:25 Task [ 0] updates[ 17000] train loss[0.29961] remaining[1:22:32]\n",
      "05/27/2019 11:20:43 Task [ 0] updates[ 17500] train loss[0.29947] remaining[1:21:16]\n",
      "05/27/2019 11:22:01 Task [ 0] updates[ 18000] train loss[0.29925] remaining[1:20:00]\n",
      "05/27/2019 11:23:18 Task [ 0] updates[ 18500] train loss[0.29978] remaining[1:18:43]\n",
      "05/27/2019 11:24:35 Task [ 0] updates[ 19000] train loss[0.29954] remaining[1:17:25]\n",
      "05/27/2019 11:25:51 Task [ 0] updates[ 19500] train loss[0.29965] remaining[1:16:07]\n",
      "05/27/2019 11:27:07 Task [ 0] updates[ 20000] train loss[0.30028] remaining[1:14:48]\n",
      "05/27/2019 11:28:24 Task [ 0] updates[ 20500] train loss[0.30106] remaining[1:13:31]\n",
      "05/27/2019 11:29:42 Task [ 0] updates[ 21000] train loss[0.30198] remaining[1:12:14]\n",
      "05/27/2019 11:30:59 Task [ 0] updates[ 21500] train loss[0.30214] remaining[1:10:57]\n",
      "05/27/2019 11:32:16 Task [ 0] updates[ 22000] train loss[0.30295] remaining[1:09:40]\n",
      "05/27/2019 11:33:33 Task [ 0] updates[ 22500] train loss[0.30344] remaining[1:08:22]\n",
      "05/27/2019 11:34:49 Task [ 0] updates[ 23000] train loss[0.30392] remaining[1:07:04]\n",
      "05/27/2019 11:36:07 Task [ 0] updates[ 23500] train loss[0.30492] remaining[1:05:47]\n",
      "05/27/2019 11:37:24 Task [ 0] updates[ 24000] train loss[0.30538] remaining[1:04:31]\n",
      "05/27/2019 11:38:42 Task [ 0] updates[ 24500] train loss[0.30599] remaining[1:03:14]\n",
      "05/27/2019 11:39:59 Task [ 0] updates[ 25000] train loss[0.30630] remaining[1:01:57]\n",
      "05/27/2019 11:41:17 Task [ 0] updates[ 25500] train loss[0.30694] remaining[1:00:40]\n",
      "05/27/2019 11:42:34 Task [ 0] updates[ 26000] train loss[0.30743] remaining[0:59:23]\n",
      "05/27/2019 11:43:52 Task [ 0] updates[ 26500] train loss[0.30819] remaining[0:58:06]\n",
      "05/27/2019 11:45:10 Task [ 0] updates[ 27000] train loss[0.30878] remaining[0:56:50]\n",
      "05/27/2019 11:46:26 Task [ 0] updates[ 27500] train loss[0.30957] remaining[0:55:32]\n",
      "05/27/2019 11:47:43 Task [ 0] updates[ 28000] train loss[0.30988] remaining[0:54:14]\n",
      "05/27/2019 11:49:00 Task [ 0] updates[ 28500] train loss[0.30998] remaining[0:52:57]\n",
      "05/27/2019 11:50:18 Task [ 0] updates[ 29000] train loss[0.31036] remaining[0:51:41]\n",
      "05/27/2019 11:51:35 Task [ 0] updates[ 29500] train loss[0.31111] remaining[0:50:23]\n",
      "05/27/2019 11:52:52 Task [ 0] updates[ 30000] train loss[0.31141] remaining[0:49:06]\n",
      "05/27/2019 11:54:09 Task [ 0] updates[ 30500] train loss[0.31180] remaining[0:47:49]\n",
      "05/27/2019 11:55:26 Task [ 0] updates[ 31000] train loss[0.31167] remaining[0:46:31]\n",
      "05/27/2019 11:56:43 Task [ 0] updates[ 31500] train loss[0.31208] remaining[0:45:14]\n",
      "05/27/2019 11:58:01 Task [ 0] updates[ 32000] train loss[0.31253] remaining[0:43:57]\n",
      "05/27/2019 11:59:18 Task [ 0] updates[ 32500] train loss[0.31305] remaining[0:42:40]\n",
      "05/28/2019 12:00:35 Task [ 0] updates[ 33000] train loss[0.31326] remaining[0:41:23]\n",
      "05/28/2019 12:01:52 Task [ 0] updates[ 33500] train loss[0.31359] remaining[0:40:05]\n",
      "05/28/2019 12:03:09 Task [ 0] updates[ 34000] train loss[0.31380] remaining[0:38:48]\n",
      "05/28/2019 12:04:27 Task [ 0] updates[ 34500] train loss[0.31405] remaining[0:37:31]\n",
      "05/28/2019 12:05:44 Task [ 0] updates[ 35000] train loss[0.31456] remaining[0:36:14]\n",
      "05/28/2019 12:07:01 Task [ 0] updates[ 35500] train loss[0.31474] remaining[0:34:57]\n",
      "05/28/2019 12:08:18 Task [ 0] updates[ 36000] train loss[0.31498] remaining[0:33:40]\n",
      "05/28/2019 12:09:35 Task [ 0] updates[ 36500] train loss[0.31548] remaining[0:32:23]\n",
      "05/28/2019 12:10:53 Task [ 0] updates[ 37000] train loss[0.31585] remaining[0:31:05]\n",
      "05/28/2019 12:12:09 Task [ 0] updates[ 37500] train loss[0.31584] remaining[0:29:48]\n",
      "05/28/2019 12:13:25 Task [ 0] updates[ 38000] train loss[0.31642] remaining[0:28:31]\n",
      "05/28/2019 12:14:42 Task [ 0] updates[ 38500] train loss[0.31660] remaining[0:27:13]\n",
      "05/28/2019 12:15:59 Task [ 0] updates[ 39000] train loss[0.31701] remaining[0:25:56]\n",
      "05/28/2019 12:17:16 Task [ 0] updates[ 39500] train loss[0.31710] remaining[0:24:39]\n",
      "05/28/2019 12:18:33 Task [ 0] updates[ 40000] train loss[0.31746] remaining[0:23:22]\n",
      "05/28/2019 12:19:50 Task [ 0] updates[ 40500] train loss[0.31769] remaining[0:22:05]\n",
      "05/28/2019 12:21:07 Task [ 0] updates[ 41000] train loss[0.31777] remaining[0:20:47]\n",
      "05/28/2019 12:22:25 Task [ 0] updates[ 41500] train loss[0.31785] remaining[0:19:30]\n",
      "05/28/2019 12:23:42 Task [ 0] updates[ 42000] train loss[0.31809] remaining[0:18:13]\n",
      "05/28/2019 12:25:00 Task [ 0] updates[ 42500] train loss[0.31838] remaining[0:16:56]\n",
      "05/28/2019 12:26:17 Task [ 0] updates[ 43000] train loss[0.31873] remaining[0:15:39]\n",
      "05/28/2019 12:27:35 Task [ 0] updates[ 43500] train loss[0.31927] remaining[0:14:22]\n",
      "05/28/2019 12:28:52 Task [ 0] updates[ 44000] train loss[0.31972] remaining[0:13:05]\n",
      "05/28/2019 12:30:10 Task [ 0] updates[ 44500] train loss[0.31993] remaining[0:11:48]\n",
      "05/28/2019 12:31:26 Task [ 0] updates[ 45000] train loss[0.32014] remaining[0:10:30]\n",
      "05/28/2019 12:32:44 Task [ 0] updates[ 45500] train loss[0.32013] remaining[0:09:13]\n",
      "05/28/2019 12:34:01 Task [ 0] updates[ 46000] train loss[0.32056] remaining[0:07:56]\n",
      "05/28/2019 12:35:17 Task [ 0] updates[ 46500] train loss[0.32071] remaining[0:06:39]\n",
      "05/28/2019 12:36:35 Task [ 0] updates[ 47000] train loss[0.32092] remaining[0:05:22]\n",
      "05/28/2019 12:37:52 Task [ 0] updates[ 47500] train loss[0.32151] remaining[0:04:05]\n",
      "05/28/2019 12:39:10 Task [ 0] updates[ 48000] train loss[0.32148] remaining[0:02:47]\n",
      "05/28/2019 12:40:27 Task [ 0] updates[ 48500] train loss[0.32194] remaining[0:01:30]\n",
      "05/28/2019 12:41:45 Task [ 0] updates[ 49000] train loss[0.32224] remaining[0:00:13]\n",
      "05/28/2019 12:42:34 Task mnli_mismatched -- epoch 0 -- Dev ACC: 83.706\n",
      "05/28/2019 12:43:09 [new test scores saved.]\n",
      "05/28/2019 12:43:44 Task mnli_matched -- epoch 0 -- Dev ACC: 83.617\n",
      "05/28/2019 12:44:19 [new test scores saved.]\n",
      "05/28/2019 12:44:29 At epoch 1\n",
      "05/28/2019 12:45:31 Task [ 0] updates[ 49500] train loss[0.32189] remaining[2:03:39]\n",
      "05/28/2019 12:46:48 Task [ 0] updates[ 50000] train loss[0.32124] remaining[2:02:46]\n",
      "05/28/2019 12:48:05 Task [ 0] updates[ 50500] train loss[0.32074] remaining[2:01:41]\n",
      "05/28/2019 12:49:22 Task [ 0] updates[ 51000] train loss[0.32030] remaining[2:00:27]\n",
      "05/28/2019 12:50:39 Task [ 0] updates[ 51500] train loss[0.31999] remaining[1:59:18]\n",
      "05/28/2019 12:51:56 Task [ 0] updates[ 52000] train loss[0.31976] remaining[1:58:20]\n",
      "05/28/2019 12:53:13 Task [ 0] updates[ 52500] train loss[0.31918] remaining[1:57:01]\n",
      "05/28/2019 12:54:31 Task [ 0] updates[ 53000] train loss[0.31891] remaining[1:55:50]\n",
      "05/28/2019 12:55:48 Task [ 0] updates[ 53500] train loss[0.31848] remaining[1:54:42]\n",
      "05/28/2019 12:57:06 Task [ 0] updates[ 54000] train loss[0.31796] remaining[1:53:26]\n",
      "05/28/2019 12:58:22 Task [ 0] updates[ 54500] train loss[0.31776] remaining[1:52:07]\n",
      "05/28/2019 12:59:39 Task [ 0] updates[ 55000] train loss[0.31751] remaining[1:50:51]\n",
      "05/28/2019 01:00:56 Task [ 0] updates[ 55500] train loss[0.31717] remaining[1:49:30]\n",
      "05/28/2019 01:02:13 Task [ 0] updates[ 56000] train loss[0.31711] remaining[1:48:13]\n",
      "05/28/2019 01:03:30 Task [ 0] updates[ 56500] train loss[0.31674] remaining[1:46:59]\n",
      "05/28/2019 01:04:48 Task [ 0] updates[ 57000] train loss[0.31666] remaining[1:45:43]\n",
      "05/28/2019 01:06:04 Task [ 0] updates[ 57500] train loss[0.31662] remaining[1:44:23]\n",
      "05/28/2019 01:07:20 Task [ 0] updates[ 58000] train loss[0.31611] remaining[1:43:04]\n",
      "05/28/2019 01:08:38 Task [ 0] updates[ 58500] train loss[0.31577] remaining[1:41:47]\n",
      "05/28/2019 01:09:54 Task [ 0] updates[ 59000] train loss[0.31580] remaining[1:40:29]\n",
      "05/28/2019 01:11:12 Task [ 0] updates[ 59500] train loss[0.31553] remaining[1:39:16]\n",
      "05/28/2019 01:12:29 Task [ 0] updates[ 60000] train loss[0.31540] remaining[1:38:00]\n",
      "05/28/2019 01:13:46 Task [ 0] updates[ 60500] train loss[0.31502] remaining[1:36:43]\n",
      "05/28/2019 01:15:03 Task [ 0] updates[ 61000] train loss[0.31485] remaining[1:35:24]\n",
      "05/28/2019 01:16:19 Task [ 0] updates[ 61500] train loss[0.31474] remaining[1:34:05]\n",
      "05/28/2019 01:17:36 Task [ 0] updates[ 62000] train loss[0.31460] remaining[1:32:48]\n",
      "05/28/2019 01:18:54 Task [ 0] updates[ 62500] train loss[0.31458] remaining[1:31:32]\n",
      "05/28/2019 01:20:11 Task [ 0] updates[ 63000] train loss[0.31451] remaining[1:30:15]\n",
      "05/28/2019 01:21:28 Task [ 0] updates[ 63500] train loss[0.31460] remaining[1:28:59]\n",
      "05/28/2019 01:22:44 Task [ 0] updates[ 64000] train loss[0.31430] remaining[1:27:40]\n",
      "05/28/2019 01:24:01 Task [ 0] updates[ 64500] train loss[0.31395] remaining[1:26:23]\n",
      "05/28/2019 01:25:17 Task [ 0] updates[ 65000] train loss[0.31376] remaining[1:25:05]\n",
      "05/28/2019 01:26:35 Task [ 0] updates[ 65500] train loss[0.31375] remaining[1:23:49]\n",
      "05/28/2019 01:27:52 Task [ 0] updates[ 66000] train loss[0.31368] remaining[1:22:32]\n",
      "05/28/2019 01:29:09 Task [ 0] updates[ 66500] train loss[0.31349] remaining[1:21:16]\n",
      "05/28/2019 01:30:27 Task [ 0] updates[ 67000] train loss[0.31347] remaining[1:20:00]\n",
      "05/28/2019 01:31:43 Task [ 0] updates[ 67500] train loss[0.31332] remaining[1:18:42]\n",
      "05/28/2019 01:33:00 Task [ 0] updates[ 68000] train loss[0.31323] remaining[1:17:24]\n",
      "05/28/2019 01:34:16 Task [ 0] updates[ 68500] train loss[0.31293] remaining[1:16:07]\n",
      "05/28/2019 01:35:34 Task [ 0] updates[ 69000] train loss[0.31302] remaining[1:14:51]\n",
      "05/28/2019 01:36:50 Task [ 0] updates[ 69500] train loss[0.31282] remaining[1:13:32]\n",
      "05/28/2019 01:38:07 Task [ 0] updates[ 70000] train loss[0.31283] remaining[1:12:15]\n",
      "05/28/2019 01:39:23 Task [ 0] updates[ 70500] train loss[0.31275] remaining[1:10:58]\n",
      "05/28/2019 01:40:41 Task [ 0] updates[ 71000] train loss[0.31269] remaining[1:09:41]\n",
      "05/28/2019 01:41:57 Task [ 0] updates[ 71500] train loss[0.31252] remaining[1:08:24]\n",
      "05/28/2019 01:43:14 Task [ 0] updates[ 72000] train loss[0.31262] remaining[1:07:07]\n"
     ]
    }
   ],
   "source": [
    "!python train.py --init_checkpoint mt_dnn_models/mt_dnn_base.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripts/run_mt_dnn.sh: 2: scripts/run_mt_dnn.sh: [[: not found\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Namespace(answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=1, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=32, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='../data/mt_dnn', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, epochs=5, freeze_layers=-1, global_grad_clipping=1.0, grad_clipping=0.0, have_lr_scheduler=True, init_checkpoint='../mt_dnn_models/bert_model_large.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='checkpoints/mt-dnn-rte_adamax_answer_opt1_gc0_ggc1_2019-05-18T0717/log.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, momentum=0, mtl_opt=0, multi_gpu_on=True, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoints/mt-dnn-rte_adamax_answer_opt1_gc0_ggc1_2019-05-18T0717', pw_tasks=['qnnli'], ratio=0, scheduler_type='ms', seed=2018, task_config_path='configs/tasks_config.json', test_datasets=['mnli_matched', 'mnli_mismatched', 'rte'], train_datasets=['mnli', 'rte', 'qqp', 'qnli', 'mrpc', 'sst', 'cola', 'stsb'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)\n",
      "05/18/2019 07:17:19 1\n",
      "05/18/2019 07:17:19 Launching the MT-DNN training\n",
      "05/18/2019 07:17:19 Loading ../data/mt_dnn/mnli_train.json as task 0\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 353, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 181, in main\n",
      "    train_data = BatchGen(BatchGen.load(train_path, True, pairwise=pw_task, maxlen=args.max_seq_len),\n",
      "  File \"/home/jupyter/mt-dnn/mt_dnn/batcher.py\", line 52, in load\n",
      "    with open(path, 'r', encoding='utf-8') as reader:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/mt_dnn/mnli_train.json'\n"
     ]
    }
   ],
   "source": [
    "#!sh scripts/run_mt_dnn.sh 32 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripts/run_stsb.sh: 2: scripts/run_stsb.sh: [[: not found\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Namespace(answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=16, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='data/mt_dnn', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, epochs=5, freeze_layers=-1, global_grad_clipping=1.0, grad_clipping=0.0, have_lr_scheduler=True, init_checkpoint='mt_dnn_models/mt_dnn_large.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='checkpoints/mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-28T0247/log.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoints/mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-28T0247', pw_tasks=['qnnli'], ratio=0, scheduler_type='ms', seed=2018, task_config_path='configs/tasks_config.json', test_datasets=['stsb'], train_datasets=['stsb'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)\n",
      "05/28/2019 02:47:27 0\n",
      "05/28/2019 02:47:27 Launching the MT-DNN training\n",
      "05/28/2019 02:47:27 Loading data/mt_dnn/stsb_train.json as task 0\n",
      "Loaded 5749 samples out of 5749\n",
      "05/28/2019 02:47:27 1\n",
      "Loaded 1500 samples out of 1500\n",
      "Loaded 1379 samples out of 1379\n",
      "05/28/2019 02:47:27 ####################\n",
      "05/28/2019 02:47:27 {'log_file': 'checkpoints/mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-28T0247/log.log', 'init_checkpoint': 'mt_dnn_models/mt_dnn_large.pt', 'data_dir': 'data/mt_dnn', 'data_sort_on': False, 'name': 'farmer', 'train_datasets': ['stsb'], 'test_datasets': ['stsb'], 'pw_tasks': ['qnnli'], 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': [0], 'label_size': '1', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'cuda': True, 'log_per_updates': 500, 'epochs': 5, 'batch_size': 16, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0.0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'ema_opt': 0, 'ema_gamma': 0.995, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': 'checkpoints/mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-28T0247', 'seed': 2018, 'task_config_path': 'configs/tasks_config.json', 'tasks_dropout_p': [0.1]}\n",
      "05/28/2019 02:47:27 ####################\n",
      "05/28/2019 02:47:37 \n",
      "############# Model Arch of MT-DNN #############\n",
      "SANBertNetwork(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 1024)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (12): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (13): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (14): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (15): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (16): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (17): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (18): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (19): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (20): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (21): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (22): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (23): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (scoring_list): ModuleList(\n",
      "    (0): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "05/28/2019 02:47:37 Total number of params: 335142913\n",
      "05/28/2019 02:47:40 At epoch 0\n",
      "05/28/2019 02:47:41 Task [ 0] updates[     1] train loss[9.74833] remaining[0:04:01]\n",
      "05/28/2019 02:50:58 Task stsb -- epoch 0 -- Dev Pearson: 89.552\n",
      "05/28/2019 02:50:58 Task stsb -- epoch 0 -- Dev Spearman: 89.433\n",
      "05/28/2019 02:51:07 [new test scores saved.]\n",
      "05/28/2019 02:51:36 At epoch 1\n",
      "05/28/2019 02:52:50 Task [ 0] updates[   500] train loss[0.96010] remaining[0:01:55]\n",
      "05/28/2019 02:54:54 Task stsb -- epoch 1 -- Dev Pearson: 89.832\n",
      "05/28/2019 02:54:54 Task stsb -- epoch 1 -- Dev Spearman: 90.095\n",
      "05/28/2019 02:55:03 [new test scores saved.]\n",
      "05/28/2019 02:55:26 At epoch 2\n",
      "05/28/2019 02:57:57 Task [ 0] updates[  1000] train loss[0.57005] remaining[0:00:42]\n",
      "05/28/2019 02:58:48 Task stsb -- epoch 2 -- Dev Pearson: 90.456\n",
      "05/28/2019 02:58:48 Task stsb -- epoch 2 -- Dev Spearman: 90.353\n",
      "05/28/2019 02:58:57 [new test scores saved.]\n",
      "05/28/2019 02:59:20 At epoch 3\n",
      "05/28/2019 03:02:42 Task stsb -- epoch 3 -- Dev Pearson: 90.362\n",
      "05/28/2019 03:02:42 Task stsb -- epoch 3 -- Dev Spearman: 90.134\n",
      "05/28/2019 03:02:52 [new test scores saved.]\n",
      "05/28/2019 03:03:14 At epoch 4\n",
      "05/28/2019 03:03:47 Task [ 0] updates[  1500] train loss[0.41430] remaining[0:02:36]\n",
      "05/28/2019 03:06:35 Task stsb -- epoch 4 -- Dev Pearson: 90.548\n",
      "05/28/2019 03:06:35 Task stsb -- epoch 4 -- Dev Spearman: 90.377\n",
      "05/28/2019 03:06:44 [new test scores saved.]\n"
     ]
    }
   ],
   "source": [
    "!sh scripts/run_stsb.sh 16 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh scripts/run_sst.sh 32 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_data_proc_512.log\tdocker\t       mt-dnn-train.log  scripts\n",
      "checkpoint\t\tdownload.sh    prepro.py\t TEST.txt\n",
      "checkpoints\t\tLICENSE        project.ipynb\t train.py\n",
      "config\t\t\tmodule\t       README.md\n",
      "data\t\t\tmt_dnn\t       requirements.txt\n",
      "data_utils\t\tmt_dnn_models  run_toy.sh\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mt-dnn/checkpoints\n"
     ]
    }
   ],
   "source": [
    "%cd checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mt-dnn-rte_adamax_answer_opt1_gc0_ggc1_2019-05-18T0717\n",
      "mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-18T0650\n",
      "mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-27T0125\n",
      "mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-28T0238\n",
      "mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-28T0246\n",
      "mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-28T0247\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "..\n",
      "mt-dnn-rte_adamax_answer_opt1_gc0_ggc1_2019-05-18T0717\n",
      "mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-18T0650\n",
      "mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-27T0125\n",
      "mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-28T0238\n",
      "mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-28T0246\n",
      "mt-dnn-stsb_adamax_answer_opt0_gc0_ggc1_2019-05-28T0247\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
